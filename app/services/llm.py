import json
import logging
import re
from collections import Counter
from typing import List, Optional

from loguru import logger
from openai import AzureOpenAI, OpenAI

from app.config import config

_max_retries = 3

def _generate_response(prompt: str) -> str:
    config.reload()
    
    content = ""
    llm_provider = config.app.get("llm_provider", "gemini")
    logger.info(f"llm provider: {llm_provider}")
    
    # Multi-API key rotation for better quota management
    if llm_provider == "gemini":
        # Collect all available Gemini API keys (up to 5 keys)
        gemini_keys = []
        for i in range(1, 6):  # gemini_api_key, gemini_api_key_2, ..., gemini_api_key_5
            if i == 1:
                key = config.app.get("gemini_api_key")
            else:
                key = config.app.get(f"gemini_api_key_{i}")
            if key:
                gemini_keys.append((i, key))
        
        if not gemini_keys:
            logger.error("No Gemini API keys configured")
            raise Exception("No Gemini API keys available")
        
        logger.info(f"Found {len(gemini_keys)} Gemini API keys for rotation")
        
        # Try each key until one works
        for key_num, api_key in gemini_keys:
            try:
                logger.info(f"Trying Gemini API key #{key_num} ({len(gemini_keys)} total)")
                import google.generativeai as genai
                genai.configure(api_key=api_key)
                
                model_name = config.app.get("gemini_model_name", "gemini-2.5-flash")
                target_model = model_name.strip()
                models_to_try = [
                    target_model,
                    "gemini-2.5-flash",
                    "gemini-flash-latest",
                    "gemini-2.0-flash",
                    "gemini-2.0-flash-exp",
                ]
                
                for m in models_to_try:
                    try:
                        logger.info(f"Using Gemini model: {m} with API key #{key_num}")
                        model = genai.GenerativeModel(m)
                        response = model.generate_content(prompt)
                        if response and getattr(response, "text", None):
                            logger.success(f"âœ… Success with API key #{key_num} and model {m}")
                            return response.text
                    except Exception as e_try:
                        logger.warning(f"Gemini model {m} failed with key #{key_num}: {e_try}")
                        if "429" in str(e_try) or "Quota exceeded" in str(e_try) or "Resource has been exhausted" in str(e_try) or "RESOURCE_EXHAUSTED" in str(e_try):
                            logger.warning(f"ğŸš« API key #{key_num} quota exceeded, trying next key...")
                            break
                        continue
                        
            except Exception as e:
                logger.warning(f"âŒ Gemini API key #{key_num} failed: {e}")
                if "429" in str(e) or "Quota exceeded" in str(e) or "Resource has been exhausted" in str(e) or "RESOURCE_EXHAUSTED" in str(e):
                    logger.warning(f"ğŸš« API key #{key_num} quota exceeded")
                continue
        
        # If all Gemini keys fail, fall back to DeepSeek
        logger.error("âŒ All Gemini API keys failed or quota exceeded, falling back to DeepSeek")
        llm_provider = "deepseek"
    
    # Remove the redirect to gemini for g4f
    if llm_provider in ["pollinations", "free"]:
        llm_provider = "gemini"

    # Standard Providers
    try:
        api_key = config.app.get(f"{llm_provider}_api_key")
        base_url = config.app.get(f"{llm_provider}_base_url")
        model_name = config.app.get(f"{llm_provider}_model_name")
        
        if llm_provider == "openai":
            client = OpenAI(api_key=api_key, base_url=base_url)
            response = client.chat.completions.create(
                model=model_name or "gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}]
            )
            return response.choices[0].message.content
            
        elif llm_provider == "moonshot":
            client = OpenAI(api_key=api_key, base_url="https://api.moonshot.cn/v1")
            response = client.chat.completions.create(
                model=model_name or "moonshot-v1-8k",
                messages=[{"role": "user", "content": prompt}]
            )
            return response.choices[0].message.content
            
        elif llm_provider == "azure":
            client = AzureOpenAI(
                api_key=api_key,
                api_version="2024-02-15-preview",
                azure_endpoint=base_url
            )
            response = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.choices[0].message.content
            
        elif llm_provider == "gemini":
            try:
                import google.generativeai as genai
                genai.configure(api_key=api_key)
                
                target_model = (model_name or "gemini-2.5-flash").strip()
                models_to_try = [
                    target_model,
                    "gemini-2.5-flash",
                    "gemini-flash-latest",
                    "gemini-2.0-flash",
                    "gemini-2.0-flash-exp",
                ]
                
                last_error = None
                for m in models_to_try:
                    try:
                        logger.info(f"Using Gemini model: {m}")
                        model = genai.GenerativeModel(m)
                        response = model.generate_content(prompt)
                        if response and getattr(response, "text", None):
                            return response.text
                    except Exception as e_try:
                        last_error = e_try
                        logger.warning(f"Gemini model {m} failed: {e_try}")
                        if "429" in str(e_try) or "Quota exceeded" in str(e_try):
                            logger.warning("Gemini Quota Exceeded. Skipping other models.")
                            break
                        continue
                
                try:
                    logger.info("Listing available Gemini models to auto-correct...")
                    available = []
                    for m in genai.list_models():
                        if 'generateContent' in m.supported_generation_methods:
                            available.append(m.name)
                    logger.info(f"Available models: {', '.join(available[:10])}...")
                except Exception as e_list:
                    logger.warning(f"Failed to list Gemini models: {e_list}")
                
                if last_error:
                    raise last_error
            except Exception as e:
                logger.warning(f"Gemini failed ({e})")
                raise e

        # Add other providers as needed (DeepSeek, Qwen, etc usually generic OpenAI)
        elif llm_provider in ["deepseek", "qwen", "ollama", "oneapi", "cloudflare", "ernie", "modelscope"]:
             # Generic OpenAI client
             if not base_url and llm_provider == "ollama":
                 base_url = "http://localhost:11434/v1"
             
             client = OpenAI(api_key=api_key or "dummy", base_url=base_url)
             response = client.chat.completions.create(
                model=model_name or "gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}]
            )
             return response.choices[0].message.content
             
        elif llm_provider == "g4f":
            try:
                import g4f
                logger.info("Using G4F provider")
                
                # Try multiple providers in order of reliability
                providers_to_try = [
                    g4f.Provider.DuckDuckGo,
                    g4f.Provider.Blackbox,
                    g4f.Provider.Airforce,
                    g4f.Provider.DarkAI,
                ]
                
                for provider in providers_to_try:
                    try:
                        logger.info(f"Trying G4F provider: {provider.__name__}")
                        response = g4f.ChatCompletion.create(
                            model="gpt-3.5-turbo",
                            provider=provider,
                            messages=[{"role": "user", "content": prompt}]
                        )
                        if response and response.strip():
                            logger.info(f"G4F success with {provider.__name__}")
                            return response.strip()
                    except Exception as e:
                        logger.warning(f"G4F provider {provider.__name__} failed: {e}")
                        continue
                
                # If all providers fail, raise the last error
                raise Exception("All G4F providers failed")
                
            except ImportError:
                logger.error("G4F not installed. Install with: pip install g4f")
                raise Exception("G4F not installed")
            except Exception as e:
                logger.error(f"G4F error: {e}")
                raise e

    except Exception as e:
        logger.error(f"LLM {llm_provider} error: {e}")
        raise e
        
    return ""

def generate_script(video_subject: str, language: str = "auto", paragraph_number: int = 1) -> str:
    # ì–¸ì–´ë³„ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    if language == "ko-KR" or language == "auto":
        # paragraph_numberì— ë”°ë¥¸ ëŒ€ë³¸ ê¸¸ì´ ë™ì  ì„¤ì •
        if paragraph_number >= 3:
            length_description = "ì´ ê¸¸ì´ëŠ” 90-120ì´ˆ ë¶„ëŸ‰ (ì•½ 300-400ì)"
            detail_instruction = "ê° ë¬¸ë‹¨ì€ 3-4ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ ì¶©ë¶„íˆ ìƒì„¸í•˜ê²Œ ì‘ì„±"
        elif paragraph_number >= 2:
            length_description = "ì´ ê¸¸ì´ëŠ” 60-90ì´ˆ ë¶„ëŸ‰ (ì•½ 200-280ì)"
            detail_instruction = "ê° ë¬¸ë‹¨ì€ 3ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ ìƒì„¸í•˜ê²Œ ì‘ì„±"
        else:
            length_description = "ì´ ê¸¸ì´ëŠ” 30-60ì´ˆ ë¶„ëŸ‰ (ì•½ 150-200ì)"
            detail_instruction = "ê° ë¬¸ë‹¨ì€ 2-3ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±"
        
        prompt = f"""
        ì£¼ì œ '{video_subject}'ì— ëŒ€í•œ ìœ íŠœë¸Œ ì‡¼ì¸ ìš© ëŒ€ë³¸ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

        âš ï¸ ì¤‘ìš”: ë°˜ë“œì‹œ ì•„ë˜ ê¸¸ì´ ìš”êµ¬ì‚¬í•­ì„ ì¤€ìˆ˜í•´ì£¼ì„¸ìš”!

        ìš”êµ¬ì‚¬í•­:
        1. {paragraph_number}ê°œì˜ ë¬¸ë‹¨ìœ¼ë¡œ êµ¬ì„±
        2. {detail_instruction}
        3. {length_description}
        4. ì¸ì‚¬ë§(ì•ˆë…•í•˜ì„¸ìš”, ì—¬ëŸ¬ë¶„, ì‹œì²­ì ì—¬ëŸ¬ë¶„ ë“±) ì‚¬ìš© ê¸ˆì§€
        5. ë°”ë¡œ ë³¸ë¡ ìœ¼ë¡œ ì‹œì‘
        6. êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ë‚´ìš©ì„ í’ë¶€í•˜ê²Œ í¬í•¨
        7. ê°ì •ì ì´ê³  ë§¤ë ¥ì ì¸ í‘œí˜„ ì‚¬ìš©
        8. ì‹œì²­ìì˜ ê´€ì‹¬ì„ ëŒ ìˆ˜ ìˆëŠ” ë‚´ìš©
        9. ë§ˆí¬ë‹¤ìš´ í˜•ì‹(**, ##, - ë“±) ì‚¬ìš© ê¸ˆì§€
        10. ì¥ë©´ ì„¤ëª…([ì¥ë©´ 1] ë“±) ì‚¬ìš© ê¸ˆì§€
        11. ìˆœìˆ˜í•œ í…ìŠ¤íŠ¸ë§Œ ì‘ì„±
        12. âš ï¸ ë„ˆë¬´ ì§§ê²Œ ì‘ì„±í•˜ì§€ ë§ê³  ì¶©ë¶„í•œ ê¸¸ì´ë¡œ ì‘ì„±í•  ê²ƒ

        ìŠ¤íƒ€ì¼: ì§ì ‘ì ì´ê³  ì„íŒ©íŠ¸ ìˆê²Œ, ì¸ì‚¬ë§ ì—†ì´ ë°”ë¡œ í•µì‹¬ ë‚´ìš©ìœ¼ë¡œ ì‹œì‘

        ì£¼ì œ: {video_subject}

        âš ï¸ ë‹¤ì‹œ í•œë²ˆ ê°•ì¡°: {length_description}ì— ë§ê²Œ ì¶©ë¶„íˆ ê¸¸ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”!

        ëŒ€ë³¸ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
        """
    else:
        # paragraph_numberì— ë”°ë¥¸ ì˜ì–´ ëŒ€ë³¸ ê¸¸ì´ ë™ì  ì„¤ì •
        if paragraph_number >= 3:
            length_description = "Total length: 90-120 seconds (about 250-350 words)"
            detail_instruction = "Each paragraph: 3-4 sentences with detailed content"
        elif paragraph_number >= 2:
            length_description = "Total length: 60-90 seconds (about 180-250 words)"
            detail_instruction = "Each paragraph: 3 sentences with detailed content"
        else:
            length_description = "Total length: 30-60 seconds (about 120-180 words)"
            detail_instruction = "Each paragraph: 2-3 sentences"
        
        prompt = f"""
        Write a YouTube Shorts script about '{video_subject}'.

        âš ï¸ IMPORTANT: Please strictly follow the length requirements below!

        Requirements:
        1. {paragraph_number} paragraphs
        2. {detail_instruction}
        3. {length_description}
        4. NO greetings (Hello, Hi everyone, Welcome, etc.)
        5. Start directly with the main content
        6. Include specific and practical information with rich details
        7. Engaging and emotional language
        8. Content that captures viewer attention
        9. NO markdown formatting (**, ##, - etc.)
        10. NO scene descriptions ([Scene 1] etc.)
        11. Plain text only
        12. âš ï¸ Do NOT write too short - write with sufficient length

        Style: Direct and impactful, start immediately with core content

        Subject: {video_subject}

        âš ï¸ REMINDER: Please write according to {length_description} with sufficient length!

        Write the script:
        """
    
    # Check if subject is empty
    if not video_subject:
        return ""

    final_script = ""
    for i in range(_max_retries):
        try:
            response = _generate_response(prompt)
            if response:
                script = response.strip()
                
                # ì¸ì‚¬ë§ ì œê±° ë¡œì§
                script = _remove_greetings(script, language)
                
                # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì œê±°
                script = _clean_markdown_formatting(script)
                
                final_script = script
                break
        except Exception as e:
            logger.error(f"failed to generate script: {e}")
            
    if not final_script:
        return "AI ëŒ€ë³¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
        
    return final_script

def generate_terms(video_subject: str, video_script: str, amount: int = 5) -> List[str]:
    logger.info(f"Starting enhanced script-content matching keyword generation for subject: {video_subject}")
    
    # ëŒ€ë³¸ ë‚´ìš©ì„ ë¬¸ì¥ë³„ë¡œ ë¶„ì„í•˜ì—¬ ë” ì •í™•í•œ í‚¤ì›Œë“œ ìƒì„±
    prompt = f"""
    You are a professional video editor specializing in matching script content with perfect background footage.

    TASK: Analyze the script sentence by sentence and generate English keywords that will find EXACTLY matching stock footage.

    ANALYSIS REQUIREMENTS:
    1. Read each sentence and identify SPECIFIC visual elements mentioned
    2. Convert Korean concepts to English keywords that stock footage sites use
    3. Focus on CONCRETE, FILMABLE elements (not abstract concepts)
    4. Each keyword should be 1-2 words maximum
    5. Prioritize keywords that will find footage matching the script's narrative flow
    6. Avoid generic terms - be SPECIFIC to what's actually mentioned

    SCRIPT CONTENT ANALYSIS:
    Subject: {video_subject}
    Full Script: {video_script}

    DETAILED ANALYSIS PROCESS:
    1. Break down the script into key visual concepts
    2. Identify specific actions, objects, settings, emotions that can be filmed
    3. Convert to English terms commonly used in stock footage
    4. Ensure keywords match the script's tone and message

    EXAMPLES OF GOOD KEYWORD MATCHING:
    - Script mentions "ì•„ì¹¨ ë£¨í‹´" â†’ keywords: "morning routine", "wake up", "coffee"
    - Script mentions "ì„±ê³µí•œ ì‚¬ëŒë“¤" â†’ keywords: "business success", "professional", "achievement"
    - Script mentions "ìš´ë™í•˜ëŠ” ë°©ë²•" â†’ keywords: "workout", "exercise", "fitness training"
    - Script mentions "ëˆ ê´€ë¦¬" â†’ keywords: "money management", "finance", "savings"

    Based on the script analysis above, generate {amount} HIGHLY SPECIFIC English keywords that will find footage perfectly matching the script content.
    
    Return ONLY the keywords separated by commas, no explanations:
    """
    
    try:
        logger.info("Generating script-content-matched keywords using advanced LLM analysis...")
        response = _generate_response(prompt)
        logger.info(f"LLM keyword response: {response}")
        
        if response:
            # Clean up the response more aggressively
            cleaned = response.strip()
            
            # Remove common prefixes and formatting
            prefixes = ["keywords:", "Keywords:", "Sure", "The keywords", "Based on", "For this", "Here are", "Analysis:", "Result:"]
            for p in prefixes:
                if cleaned.lower().startswith(p.lower()):
                    cleaned = cleaned[len(p):].strip()
                    if cleaned.startswith(":"):
                        cleaned = cleaned[1:].strip()
            
            # Clean formatting and extract terms
            cleaned = cleaned.replace("\n", ",").replace("- ", "").replace("* ", "").replace('"', '').replace("'", "").strip()
            terms = [t.strip() for t in cleaned.split(",") if t.strip()]
            
            # Enhanced validation for script-content matching
            valid_terms = []
            for term in terms:
                term = term.strip().lower()
                # More strict validation for content matching
                if (len(term.split()) <= 2 and  # Max 2 words for better search results
                    term.isascii() and 
                    len(term) > 2 and
                    # Exclude generic/meta terms
                    not any(generic in term for generic in [
                        "ai generated", "viral", "content", "shorts", "video", "youtube",
                        "script", "keyword", "analysis", "example", "concept"
                    ]) and
                    # Exclude common stop words
                    not any(stop_word in term for stop_word in [
                        "the", "and", "or", "but", "with", "for", "this", "that", "these", "those"
                    ]) and
                    # Ensure it's a concrete, searchable term
                    not term.startswith(("how to", "what is", "why", "when"))):
                    valid_terms.append(term)
            
            logger.info(f"Script-matched valid terms: {valid_terms}")
            
            if len(valid_terms) >= 3:
                # Return the most relevant terms
                final_terms = valid_terms[:amount]
                logger.info(f"Final script-content-matched keywords: {final_terms}")
                return final_terms
                
    except Exception as e:
        logger.error(f"Failed to generate script-content-matched terms: {e}")
    
    # Enhanced fallback with deeper script analysis
    logger.warning("LLM failed. Using enhanced script content analysis fallback...")
    return _generate_enhanced_script_keywords(video_subject, video_script, amount)


def _generate_enhanced_script_keywords(video_subject: str, video_script: str, amount: int = 5) -> List[str]:
    """Enhanced fallback that analyzes script content more deeply"""
    logger.info(f"Performing enhanced script content analysis for: '{video_subject}'")
    
    script_lower = video_script.lower()
    subject_lower = video_subject.lower()
    
    # Comprehensive Korean-to-English mapping with script context analysis
    content_mapping = {
        # Actions and behaviors
        "ë£¨í‹´": ["routine", "daily habits"],
        "ìŠµê´€": ["habits", "lifestyle"],
        "ë°©ë²•": ["method", "technique"],
        "ë¹„ë²•": ["secret", "strategy"],
        "íŒ": ["tips", "advice"],
        "ê°€ì´ë“œ": ["guide", "tutorial"],
        "ìš´ë™": ["workout", "exercise"],
        "ëª…ìƒ": ["meditation", "mindfulness"],
        "ì§‘ì¤‘": ["focus", "concentration"],
        "ê³µë¶€": ["study", "learning"],
        "ë…ì„œ": ["reading", "books"],
        "ìš”ë¦¬": ["cooking", "kitchen"],
        "ì²­ì†Œ": ["cleaning", "organization"],
        
        # Emotions and states
        "ì„±ê³µ": ["success", "achievement"],
        "í–‰ë³µ": ["happiness", "joy"],
        "ìŠ¤íŠ¸ë ˆìŠ¤": ["stress", "pressure"],
        "í”¼ë¡œ": ["tired", "exhausted"],
        "ì—ë„ˆì§€": ["energy", "vitality"],
        "ìì‹ ê°": ["confidence", "self-esteem"],
        "ë™ê¸°": ["motivation", "inspiration"],
        
        # Objects and tools
        "ëˆ": ["money", "cash"],
        "íˆ¬ì": ["investment", "finance"],
        "ë¶€ë™ì‚°": ["real estate", "property"],
        "ìë™ì°¨": ["car", "vehicle"],
        "í•¸ë“œí°": ["smartphone", "phone"],
        "ì»´í“¨í„°": ["computer", "laptop"],
        "ì±…": ["book", "reading"],
        "ìŒì‹": ["food", "meal"],
        "ì»¤í”¼": ["coffee", "cafe"],
        
        # Places and environments
        "ì§‘": ["home", "house"],
        "ì‚¬ë¬´ì‹¤": ["office", "workplace"],
        "ì¹´í˜": ["cafe", "coffee shop"],
        "í—¬ìŠ¤ì¥": ["gym", "fitness"],
        "ë„ì„œê´€": ["library", "study"],
        "ê³µì›": ["park", "outdoor"],
        "ë°”ë‹¤": ["ocean", "beach"],
        "ì‚°": ["mountain", "hiking"],
        
        # Time and scheduling
        "ì•„ì¹¨": ["morning", "sunrise"],
        "ì €ë…": ["evening", "sunset"],
        "ë°¤": ["night", "dark"],
        "ì£¼ë§": ["weekend", "leisure"],
        "íœ´ê°€": ["vacation", "holiday"],
        "ì‹œê°„": ["time", "clock"],
        
        # Health and wellness
        "ê±´ê°•": ["health", "wellness"],
        "ë‹¤ì´ì–´íŠ¸": ["diet", "weight loss"],
        "ì˜ì–‘": ["nutrition", "healthy food"],
        "ìˆ˜ë©´": ["sleep", "rest"],
        "íœ´ì‹": ["rest", "relaxation"],
        
        # Relationships and social
        "ê°€ì¡±": ["family", "together"],
        "ì¹œêµ¬": ["friends", "social"],
        "ì—°ì¸": ["couple", "romance"],
        "ê²°í˜¼": ["wedding", "marriage"],
        "ì•„ì´": ["children", "kids"],
        "ë¶€ëª¨": ["parents", "family"],
        
        # Work and career
        "ì¼": ["work", "business"],
        "íšŒì‚¬": ["company", "corporate"],
        "ì°½ì—…": ["startup", "entrepreneur"],
        "ë©´ì ‘": ["interview", "job"],
        "ìŠ¹ì§„": ["promotion", "career"],
        "í‡´ì‚¬": ["resignation", "quit job"],
        
        # Technology and modern life
        "AI": ["artificial intelligence", "technology"],
        "ë””ì§€í„¸": ["digital", "tech"],
        "ì˜¨ë¼ì¸": ["online", "internet"],
        "ì†Œì…œë¯¸ë””ì–´": ["social media", "smartphone"],
        "ìœ íŠœë¸Œ": ["content creation", "video"],
        "ì¸ìŠ¤íƒ€ê·¸ë¨": ["social media", "photography"]
    }
    
    # Analyze script content for matching keywords
    matched_keywords = []
    
    # First, look for direct matches in script content
    for korean_term, english_terms in content_mapping.items():
        if korean_term in script_lower:
            matched_keywords.extend(english_terms[:2])  # Take first 2 from each match
            logger.info(f"Found script content match: '{korean_term}' -> {english_terms[:2]}")
    
    # Then, look for subject-based matches
    for korean_term, english_terms in content_mapping.items():
        if korean_term in subject_lower and korean_term not in script_lower:
            matched_keywords.extend(english_terms[:1])  # Take 1 from subject matches
            logger.info(f"Found subject match: '{korean_term}' -> {english_terms[:1]}")
    
    # Remove duplicates while preserving order
    unique_keywords = []
    seen = set()
    for keyword in matched_keywords:
        if keyword.lower() not in seen:
            unique_keywords.append(keyword.lower())
            seen.add(keyword.lower())
    
    # If we don't have enough keywords, add contextual ones
    if len(unique_keywords) < amount:
        contextual_keywords = []
        
        # Add context-based keywords based on script tone and content
        if any(word in script_lower for word in ["ì„±ê³µ", "ë‹¬ì„±", "ëª©í‘œ", "ì´ë£¨"]):
            contextual_keywords.extend(["success", "achievement", "goal"])
        if any(word in script_lower for word in ["ê±´ê°•", "ìš´ë™", "ë‹¤ì´ì–´íŠ¸", "ëª¸"]):
            contextual_keywords.extend(["health", "fitness", "wellness"])
        if any(word in script_lower for word in ["ëˆ", "íˆ¬ì", "ë¶€ì", "ê²½ì œ"]):
            contextual_keywords.extend(["money", "finance", "wealth"])
        if any(word in script_lower for word in ["ì‹œê°„", "íš¨ìœ¨", "ìƒì‚°ì„±", "ê´€ë¦¬"]):
            contextual_keywords.extend(["time", "productivity", "efficiency"])
        if any(word in script_lower for word in ["í–‰ë³µ", "ë§Œì¡±", "ê¸°ì¨", "ì¦ê±°"]):
            contextual_keywords.extend(["happiness", "joy", "satisfaction"])
        
        # Add unique contextual keywords
        for keyword in contextual_keywords:
            if keyword not in seen and len(unique_keywords) < amount:
                unique_keywords.append(keyword)
                seen.add(keyword)
    
    # Final fallback with universal keywords if still not enough
    if len(unique_keywords) < amount:
        universal_keywords = ["lifestyle", "people", "modern", "daily life", "professional"]
        for keyword in universal_keywords:
            if keyword not in seen and len(unique_keywords) < amount:
                unique_keywords.append(keyword)
                seen.add(keyword)
    
    result = unique_keywords[:amount]
    logger.info(f"Enhanced script analysis final keywords: {result}")
    return result

def _generate_fallback_keywords(video_subject: str, video_script: str, amount: int = 5) -> List[str]:
    """Generate fallback keywords when LLM fails"""
    logger.info(f"Generating fallback keywords for subject: '{video_subject}' and script length: {len(video_script)}")
    
    fallback_keywords = []
    
    # Subject-based keywords
    subject_lower = video_subject.lower()
    script_lower = video_script.lower()
    
    # Enhanced Korean to English keyword mapping
    subject_keywords = {
        "ì„±ê³µ": ["success", "achievement", "business", "winner", "celebration"],
        "ê±´ê°•": ["health", "fitness", "wellness", "exercise", "nutrition"],
        "ëˆ": ["money", "finance", "wealth", "investment", "cash"],
        "íˆ¬ì": ["investment", "trading", "finance", "stock market", "business"],
        "ë‹¤ì´ì–´íŠ¸": ["diet", "weight loss", "fitness", "healthy eating", "workout"],
        "ìš´ë™": ["exercise", "workout", "fitness", "gym", "sports"],
        "ë…ì„œ": ["reading", "books", "education", "learning", "study"],
        "ê³µë¶€": ["study", "learning", "education", "school", "knowledge"],
        "ìŒì‹": ["food", "cooking", "nutrition", "kitchen", "meal"],
        "ì—¬í–‰": ["travel", "vacation", "adventure", "tourism", "journey"],
        "ì‚¬ë‘": ["love", "relationship", "romance", "couple", "heart"],
        "ê°€ì¡±": ["family", "home", "togetherness", "children", "parents"],
        "ì¼": ["work", "office", "career", "job", "business"],
        "ìŠ¤íŠ¸ë ˆìŠ¤": ["stress", "relaxation", "meditation", "calm", "peace"],
        "í–‰ë³µ": ["happiness", "joy", "celebration", "smile", "positive"],
        "ì‹œê°„": ["time", "clock", "schedule", "planning", "productivity"],
        "ìì—°": ["nature", "landscape", "outdoor", "forest", "mountains"],
        "ê¸°ìˆ ": ["technology", "innovation", "digital", "computer", "modern"],
        "ìŒì•…": ["music", "sound", "audio", "instruments", "melody"],
        "ì˜í™”": ["movie", "cinema", "entertainment", "film", "video"],
        "ê²Œì„": ["gaming", "play", "entertainment", "fun", "competition"],
        "ìš”ë¦¬": ["cooking", "kitchen", "food", "chef", "recipe"],
        "íŒ¨ì…˜": ["fashion", "style", "clothing", "design", "trendy"],
        "ë·°í‹°": ["beauty", "skincare", "cosmetics", "makeup", "care"],
        "êµìœ¡": ["education", "learning", "teaching", "school", "knowledge"],
        "ì°½ì—…": ["startup", "business", "entrepreneur", "innovation", "success"],
        "ë¶€ë™ì‚°": ["real estate", "property", "investment", "house", "building"],
        "ìë™ì°¨": ["car", "automotive", "driving", "vehicle", "transportation"],
        "ë°˜ë ¤ë™ë¬¼": ["pet", "animal", "dog", "cat", "care"],
        "ìœ¡ì•„": ["parenting", "children", "family", "baby", "care"],
        "ê²°í˜¼": ["wedding", "marriage", "couple", "love", "ceremony"],
        "ì·¨ì—…": ["job", "career", "employment", "work", "interview"]
    }
    
    # Find matching keywords from subject
    for korean, english_list in subject_keywords.items():
        if korean in subject_lower:
            fallback_keywords.extend(english_list[:3])  # Take first 3 from each match
            logger.info(f"Found subject keyword '{korean}' -> {english_list[:3]}")
    
    # Find matching keywords from script
    for korean, english_list in subject_keywords.items():
        if korean in script_lower and korean not in subject_lower:  # Avoid duplicates
            fallback_keywords.extend(english_list[:2])  # Take fewer from script
            logger.info(f"Found script keyword '{korean}' -> {english_list[:2]}")
    
    # Add generic visual keywords based on common themes
    if not fallback_keywords:
        logger.info("No specific keywords found, using generic keywords")
        fallback_keywords = ["lifestyle", "people", "modern", "business", "success"]
    
    # Add some universal keywords that work well for stock footage
    universal_keywords = ["lifestyle", "modern", "people", "business", "professional"]
    for keyword in universal_keywords:
        if keyword not in fallback_keywords:
            fallback_keywords.append(keyword)
    
    # Remove duplicates and limit
    unique_keywords = []
    seen = set()
    for keyword in fallback_keywords:
        if keyword.lower() not in seen:
            unique_keywords.append(keyword.lower())
            seen.add(keyword.lower())
    
    # Ensure we have enough keywords
    if len(unique_keywords) < amount:
        additional_keywords = ["creative", "inspiration", "motivation", "growth", "innovation"]
        for keyword in additional_keywords:
            if keyword not in seen and len(unique_keywords) < amount:
                unique_keywords.append(keyword)
                seen.add(keyword)
    
    result = unique_keywords[:amount]
    logger.info(f"Final fallback keywords: {result}")
    return result

def generate_english_script(video_subject: str, paragraph_number: int = 4) -> str:
    """Generate English script directly (for when translation fails)"""
    logger.info(f"Generating English script for subject: {video_subject}")
    
    prompt = f"""
    Create an engaging English script for a short video (60-90 seconds) about: {video_subject}

    REQUIREMENTS:
    1. Write {paragraph_number} paragraphs
    2. Each paragraph should be 2-3 sentences
    3. Use simple, clear English suitable for global audience
    4. Make it engaging and motivational
    5. Focus on practical tips or insights
    6. Use active voice and conversational tone
    7. No special formatting, just plain text

    STYLE: Motivational, educational, accessible to international viewers
    
    Subject: {video_subject}
    
    Write the script now:
    """
    
    try:
        response = _generate_response(prompt)
        if response and len(response.strip()) > 50:
            # Clean up the response
            script = response.strip()
            
            # Remove common prefixes
            prefixes = ["Here's", "Here is", "Script:", "The script", "Sure", "Certainly"]
            for prefix in prefixes:
                if script.startswith(prefix):
                    script = script[len(prefix):].strip()
                    if script.startswith(":"):
                        script = script[1:].strip()
            
            logger.info(f"Generated English script: {script[:100]}...")
            return script
            
    except Exception as e:
        logger.error(f"Failed to generate English script: {e}")
    
    # Fallback English script template
    logger.warning("Using fallback English script template")
    fallback_script = f"""
    Today we're exploring {video_subject}. This topic can transform your daily life in meaningful ways.

    Many successful people have discovered the power of focusing on what truly matters. Small changes in your routine can lead to remarkable results over time.

    The key is to start with simple, actionable steps. Don't try to change everything at once. Instead, pick one area and commit to consistent improvement.

    Remember, every expert was once a beginner. Your journey toward {video_subject} starts with a single decision to take action today.
    """
    
    return fallback_script.strip()
    
def translate_to_english(text: str) -> str:
    if not text:
        return ""
    
    # ì´ë¯¸ ì˜ì–´ì¸ì§€ í™•ì¸ (í•œê¸€ì´ ì—†ìœ¼ë©´ ì˜ì–´ë¡œ ê°„ì£¼)
    import re
    if not re.search(r'[ê°€-í£]', text):
        return text
    
    prompt = f"Translate the following Korean text into natural English. Return ONLY the translated text without any quotes, notes, or explanations:\n\n{text}"
    
    try:
        # 1ì°¨ ì‹œë„: ë©”ì¸ LLM ì‚¬ìš©
        response = _generate_response(prompt)
        if response and response.strip() != text and not re.search(r'[ê°€-í£]', response):
            logger.info(f"Successfully translated using main LLM: '{text}' -> '{response.strip()}'")
            return response.strip()
    except Exception as e:
        logger.warning(f"Main LLM failed for translation: {e}")
    
    logger.info("Main LLM failed for translation, trying free provider fallback...")
    
    try:
        # 2ì°¨ ì‹œë„: ë¬´ë£Œ ì œê³µì ì‚¬ìš©
        response = _generate_free_response(prompt)
        if response and response.strip() != text and not re.search(r'[ê°€-í£]', response):
            logger.info(f"Successfully translated using free provider: '{text}' -> '{response.strip()}'")
            return response.strip()
    except Exception as e:
        logger.warning(f"Free provider also failed: {e}")
    
    # 3ì°¨ ì‹œë„: ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤í•‘ (ë°±ì—…ìš©)
    try:
        simple_translations = {
            "ì„±ê³µ": "success",
            "ìŠµê´€": "habits", 
            "ë°©ë²•": "methods",
            "ë¹„ë²•": "secrets",
            "íŒ": "tips",
            "ê°€ì´ë“œ": "guide",
            "ë¼ì´í”„": "life",
            "ìŠ¤íƒ€ì¼": "style",
            "ê±´ê°•": "health",
            "ë‹¤ì´ì–´íŠ¸": "diet",
            "ìš´ë™": "exercise",
            "ëª…ìƒ": "meditation",
            "ì§‘ì¤‘": "focus",
            "ì‹œê°„": "time",
            "ê´€ë¦¬": "management",
            "ëˆ": "money",
            "íˆ¬ì": "investment",
            "ë¶€ì": "rich",
            "í–‰ë³µ": "happiness",
            "ì‚¬ë‘": "love",
            "ì¸ê°„ê´€ê³„": "relationships",
            "ìì‹ ê°": "confidence",
            "ë™ê¸°ë¶€ì—¬": "motivation",
            "ì˜ê°": "inspiration",
            "ì°½ì—…": "startup",
            "ë¹„ì¦ˆë‹ˆìŠ¤": "business",
            "ë§ˆì¼€íŒ…": "marketing",
            "ë¸Œëœë”©": "branding",
            "ì†Œì…œë¯¸ë””ì–´": "social media",
            "ìœ íŠœë¸Œ": "youtube",
            "ì½˜í…ì¸ ": "content",
            "í¬ë¦¬ì—ì´í„°": "creator"
        }
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë²ˆì—­ ì‹œë„
        words = text.split()
        translated_words = []
        for word in words:
            # íŠ¹ìˆ˜ë¬¸ì ì œê±°í•˜ê³  ë§¤í•‘ í™•ì¸
            clean_word = re.sub(r'[^\wê°€-í£]', '', word)
            if clean_word in simple_translations:
                translated_words.append(simple_translations[clean_word])
            else:
                # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
                found = False
                for ko, en in simple_translations.items():
                    if ko in clean_word:
                        translated_words.append(en)
                        found = True
                        break
                if not found:
                    translated_words.append(word)
        
        if translated_words and len(translated_words) > 0:
            fallback_result = " ".join(translated_words)
            if fallback_result != text:
                logger.info(f"Using fallback translation: '{text}' -> '{fallback_result}'")
                return fallback_result
    except Exception as e:
        logger.warning(f"Fallback translation failed: {e}")
    
    logger.warning("Translation failed or API error, returning original text.")
    return text

def translate_to_korean(text: str) -> str:
    if not text:
        return ""
    prompt = f"Translate the following English text into natural Korean. Return ONLY the translated text without any quotes, notes, or explanations:\n\n{text}"
    try:
        response = _generate_response(prompt)
        if response:
            return response.strip()
    except Exception as e:
        logger.error(f"Translation failed: {e}")
    return text

def translate_terms_to_korean(terms: List[str]) -> List[str]:
    if not terms:
        return []
    joined = ", ".join([t.strip() for t in terms if t])
    if not joined:
        return []
    prompt = f"Translate the following English keywords into Korean. Return ONLY a comma-separated list with no extra words:\n\n{joined}"
    try:
        response = _generate_response(prompt)
        if response:
            cleaned = response.replace("\n", ",").strip()
            out = [t.strip() for t in cleaned.split(",") if t.strip()]
            return out
    except Exception as e:
        logger.error(f"Translation failed: {e}")
    return terms

def translate_terms_to_english(terms: List[str]) -> List[str]:
    if not terms:
        return []
    joined = ", ".join([t.strip() for t in terms if t])
    if not joined:
        return []
    prompt = f"Translate the following keywords into natural English. Return ONLY a comma-separated list with no extra words:\n\n{joined}"
    try:
        response = _generate_response(prompt)
        if response:
            cleaned = response.replace("\n", ",").strip()
            out = [t.strip() for t in cleaned.split(",") if t.strip()]
            return out
    except Exception as e:
        logger.error(f"Translation failed: {e}")
    return terms
def generate_korean_terms(video_subject: str, video_script: str, amount: int = 5) -> List[str]:
    # Korean keyword generation for YouTube tags based on script content
    prompt = f"""
    ë‹¹ì‹ ì€ ì „ë¬¸ ìœ íŠœë¸Œ í¬ë¦¬ì—ì´í„°ì…ë‹ˆë‹¤.
    
    ì‘ì—…: ë‹¤ìŒ ì˜ìƒ ëŒ€ë³¸ì„ ë¶„ì„í•˜ì—¬ {amount}ê°œì˜ í•œêµ­ì–´ YouTube íƒœê·¸ë¥¼ ìƒì„±í•˜ì„¸ìš”.
    
    ìš”êµ¬ì‚¬í•­:
    1. ëŒ€ë³¸ì—ì„œ ì‹¤ì œë¡œ ì–¸ê¸‰ëœ ë‚´ìš©ë§Œ í‚¤ì›Œë“œë¡œ ì‚¬ìš©
    2. êµ¬ì²´ì ì´ê³  ê²€ìƒ‰ ê°€ëŠ¥í•œ í•œêµ­ì–´ ìš©ì–´
    3. ê° íƒœê·¸ëŠ” 1-3ë‹¨ì–´ë¡œ êµ¬ì„±
    4. ëŒ€ë³¸ì˜ í•µì‹¬ ë©”ì‹œì§€ì™€ ì§ì ‘ ê´€ë ¨ëœ í‚¤ì›Œë“œ
    5. "AIìƒì„±", "ë°”ì´ëŸ´", "ì½˜í…ì¸ ", "ì‡¼ì¸ " ê°™ì€ ì¼ë°˜ì  íƒœê·¸ ì‚¬ìš© ê¸ˆì§€
    6. ëŒ€ë³¸ì—ì„œ ì–¸ê¸‰ëœ í–‰ë™, ë°©ë²•, ê²°ê³¼, ê°ì •ì— ì§‘ì¤‘
    
    ë¶„ì„í•  ë‚´ìš©:
    ì£¼ì œ: {video_subject}
    ëŒ€ë³¸: {video_script[:1000]}
    
    ëŒ€ë³¸ì—ì„œ ì¶”ì¶œ ê°€ëŠ¥í•œ í‚¤ì›Œë“œ ì˜ˆì‹œ:
    - ì–¸ê¸‰ëœ êµ¬ì²´ì ì¸ í–‰ë™ì´ë‚˜ ë°©ë²•
    - ì œì‹œëœ ê²°ê³¼ë‚˜ íš¨ê³¼
    - ë‚˜íƒ€ë‚˜ëŠ” ê°ì •ì´ë‚˜ ìƒíƒœ
    - ì„¤ëª…ëœ ìƒí™©ì´ë‚˜ ë¬¸ì œ
    - ì œì•ˆëœ í•´ê²°ì±…ì´ë‚˜ íŒ
    
    ëŒ€ë³¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•œ {amount}ê°œì˜ í•œêµ­ì–´ í‚¤ì›Œë“œë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë‚˜ì—´í•˜ì„¸ìš”:
    """
    
    try:
        response = _generate_response(prompt)
        if response:
            # Clean up the response
            cleaned = response.strip()
            
            # Remove common prefixes
            prefixes = ["íƒœê·¸:", "í‚¤ì›Œë“œ:", "ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤", "ìƒì„±ëœ íƒœê·¸", "ë¶„ì„ ê²°ê³¼"]
            for p in prefixes:
                if cleaned.startswith(p):
                    cleaned = cleaned[len(p):].strip()
                    if cleaned.startswith(":"):
                        cleaned = cleaned[1:].strip()
            
            # Clean formatting
            cleaned = cleaned.replace("\n", ",").replace("- ", "").replace("* ", "").replace('"', '').strip()
            
            # Extract terms
            terms = [t.strip() for t in cleaned.split(",") if t.strip()]
            
            # Filter and validate Korean terms
            valid_terms = []
            for term in terms:
                term = term.strip()
                # Check if contains Korean characters and is reasonable length
                if (len(term.split()) <= 3 and len(term) > 1 and 
                    re.search("[ê°€-í£]", term) and 
                    # Exclude generic terms
                    not any(generic in term for generic in ["AIìƒì„±", "ë°”ì´ëŸ´", "ì½˜í…ì¸ ", "ì‡¼ì¸ ", "ì˜ìƒ", "ìœ íŠœë¸Œ"])):
                    valid_terms.append(term)
            
            if len(valid_terms) >= 3:
                logger.info(f"Generated script-based Korean tags: {valid_terms[:amount]}")
                return valid_terms[:amount]
                
    except Exception as e:
        logger.error(f"failed to generate Korean terms: {e}")
    
    # Enhanced fallback with script content analysis
    logger.warning("LLM failed to generate Korean terms. Using script analysis fallback.")
    return _generate_script_based_korean_keywords(video_subject, video_script, amount)

def _generate_script_based_korean_keywords(video_subject: str, video_script: str, amount: int = 5) -> List[str]:
    """ëŒ€ë³¸ ë‚´ìš© ë¶„ì„ ê¸°ë°˜ í•œêµ­ì–´ í‚¤ì›Œë“œ ìƒì„±"""
    logger.info(f"Analyzing Korean script content for keyword generation: '{video_subject}'")
    
    import re
    
    # ëŒ€ë³¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•œ íŒ¨í„´ ë¶„ì„
    script_lower = video_script.lower()
    subject_lower = video_subject.lower()
    
    # ëŒ€ë³¸ ë‚´ìš© ê¸°ë°˜ í•œêµ­ì–´ í‚¤ì›Œë“œ ë§¤í•‘ (êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì )
    content_keywords = {
        # í–‰ë™/ë°©ë²• ê´€ë ¨
        "ë¬¼ë§ˆì‹œê¸°": ["ë¬¼ë§ˆì‹œê¸°", "ìˆ˜ë¶„ì„­ì·¨", "ê±´ê°•ìŠµê´€"],
        "ìŠ¤íŠ¸ë ˆì¹­": ["ìŠ¤íŠ¸ë ˆì¹­", "ëª¸í’€ê¸°", "ìœ ì—°ì„±"],
        "ëª…ìƒ": ["ëª…ìƒ", "ë§ˆìŒì±™ê¹€", "ì •ì‹ ê±´ê°•"],
        "í˜¸í¡": ["í˜¸í¡ë²•", "ì‹¬í˜¸í¡", "ë§ˆìŒì•ˆì •"],
        "ê±·ê¸°": ["ê±·ê¸°ìš´ë™", "ì‚°ì±…", "ìœ ì‚°ì†Œ"],
        "ìš´ë™": ["ìš´ë™ë²•", "í—¬ìŠ¤", "ì²´ë ¥ê´€ë¦¬"],
        "ìš”ë¦¬": ["ìš”ë¦¬ë²•", "ìŒì‹ë§Œë“¤ê¸°", "ê±´ê°•ì‹"],
        "ë…ì„œ": ["ë…ì„œë²•", "ì±…ì½ê¸°", "ì§€ì‹ìŠµë“"],
        "ê¸°ë¡": ["ê¸°ë¡í•˜ê¸°", "ì¼ê¸°ì“°ê¸°", "ê³„íšì„¸ìš°ê¸°"],
        
        # ê²°ê³¼/íš¨ê³¼ ê´€ë ¨
        "ì§‘ì¤‘ë ¥": ["ì§‘ì¤‘ë ¥í–¥ìƒ", "ëª°ì…", "ìƒì‚°ì„±"],
        "íš¨ìœ¨": ["íš¨ìœ¨ì„±", "ì‹œê°„ê´€ë¦¬", "ìƒì‚°ì„±"],
        "ë³€í™”": ["ë³€í™”", "ê°œì„ ", "ì„±ì¥"],
        "ì„±ê³¼": ["ì„±ê³¼", "ê²°ê³¼", "ë‹¬ì„±"],
        "ê±´ê°•": ["ê±´ê°•ê´€ë¦¬", "ì›°ë¹™", "ì²´ë ¥"],
        
        # ê°ì •/ìƒíƒœ ê´€ë ¨
        "ìŠ¤íŠ¸ë ˆìŠ¤": ["ìŠ¤íŠ¸ë ˆìŠ¤ê´€ë¦¬", "ìŠ¤íŠ¸ë ˆìŠ¤í•´ì†Œ", "ë§ˆìŒê´€ë¦¬"],
        "í–‰ë³µ": ["í–‰ë³µ", "ê¸ì •", "ë§Œì¡±"],
        "ìì‹ ê°": ["ìì‹ ê°", "ìì¡´ê°", "ë§ˆì¸ë“œ"],
        "í‰ì˜¨": ["í‰ì˜¨", "ì•ˆì •", "íœ´ì‹"],
        
        # ì‹œê°„/ë£¨í‹´ ê´€ë ¨
        "ì•„ì¹¨": ["ì•„ì¹¨ë£¨í‹´", "ëª¨ë‹", "í•˜ë£¨ì‹œì‘"],
        "ì €ë…": ["ì €ë…ë£¨í‹´", "í•˜ë£¨ë§ˆë¬´ë¦¬", "íœ´ì‹"],
        "í•˜ë£¨": ["ì¼ìƒ", "ë£¨í‹´", "ìƒí™œíŒ¨í„´"],
        "ìŠµê´€": ["ì¢‹ì€ìŠµê´€", "ìŠµê´€ë§Œë“¤ê¸°", "ë¼ì´í”„ìŠ¤íƒ€ì¼"],
        
        # êµ¬ì²´ì  ë°©ë²•/íŒ
        "ë°©ë²•": ["ë°©ë²•", "ë…¸í•˜ìš°", "íŒ"],
        "ë¹„ë²•": ["ë¹„ë²•", "ê¿€íŒ", "ë…¸í•˜ìš°"],
        "ë‹¨ê³„": ["ë‹¨ê³„ë³„", "ìˆœì„œ", "ê³¼ì •"],
        "ì›ì¹™": ["ì›ì¹™", "ë²•ì¹™", "ê¸°ì¤€"]
    }
    
    # ëŒ€ë³¸ì—ì„œ ì–¸ê¸‰ëœ í‚¤ì›Œë“œ ì°¾ê¸°
    found_keywords = []
    
    # 1. ì§ì ‘ ë§¤ì¹­
    for korean_word, korean_keywords_list in content_keywords.items():
        if korean_word in script_lower or korean_word in subject_lower:
            found_keywords.extend(korean_keywords_list[:2])  # ê° ì¹´í…Œê³ ë¦¬ì—ì„œ 2ê°œì”©
            logger.info(f"Found Korean keyword '{korean_word}' -> {korean_keywords_list[:2]}")
    
    # 2. ë¬¸ë§¥ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ê°€
    context_keywords = []
    
    # ê±´ê°• ê´€ë ¨ ë¬¸ë§¥
    if any(word in script_lower for word in ["ê±´ê°•", "ëª¸", "ì²´ë ¥", "ìš´ë™", "ë‹¤ì´ì–´íŠ¸"]):
        context_keywords.extend(["ê±´ê°•ê´€ë¦¬", "ì›°ë¹™", "ì²´ë ¥í–¥ìƒ"])
    
    # ì„±ê³µ/ìê¸°ê³„ë°œ ë¬¸ë§¥
    if any(word in script_lower for word in ["ì„±ê³µ", "ëª©í‘œ", "ë‹¬ì„±", "ìŠµê´€", "ê³„ë°œ"]):
        context_keywords.extend(["ìê¸°ê³„ë°œ", "ì„±ê³µë²•", "ëª©í‘œë‹¬ì„±"])
    
    # ì¼ìƒ/ë¼ì´í”„ìŠ¤íƒ€ì¼ ë¬¸ë§¥
    if any(word in script_lower for word in ["ì¼ìƒ", "í•˜ë£¨", "ë£¨í‹´", "ìƒí™œ", "ì‹œê°„"]):
        context_keywords.extend(["ì¼ìƒë£¨í‹´", "ë¼ì´í”„ìŠ¤íƒ€ì¼", "ì‹œê°„ê´€ë¦¬"])
    
    # ë§ˆìŒ/ì •ì‹  ê±´ê°• ë¬¸ë§¥
    if any(word in script_lower for word in ["ë§ˆìŒ", "ì •ì‹ ", "ê°ì •", "ìŠ¤íŠ¸ë ˆìŠ¤", "í–‰ë³µ"]):
        context_keywords.extend(["ë§ˆìŒê´€ë¦¬", "ì •ì‹ ê±´ê°•", "ê°ì •ì¡°ì ˆ"])
    
    # 3. í‚¤ì›Œë“œ ì¡°í•© ë° ì •ë¦¬
    all_keywords = found_keywords + context_keywords
    
    # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
    unique_keywords = []
    seen = set()
    
    for keyword in all_keywords:
        keyword_clean = keyword.strip()
        if (keyword_clean not in seen and len(keyword_clean) > 1 and
            # ì¼ë°˜ì ì¸ íƒœê·¸ ì œì™¸
            not any(generic in keyword_clean for generic in ["AIìƒì„±", "ë°”ì´ëŸ´", "ì½˜í…ì¸ ", "ì‡¼ì¸ ", "ì˜ìƒ"])):
            unique_keywords.append(keyword_clean)
            seen.add(keyword_clean)
    
    # 4. ë¶€ì¡±í•œ ê²½ìš° ì£¼ì œ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ê°€
    if len(unique_keywords) < amount:
        subject_based = _get_korean_subject_keywords(video_subject)
        for keyword in subject_based:
            if keyword not in seen and len(unique_keywords) < amount:
                unique_keywords.append(keyword)
                seen.add(keyword)
    
    result = unique_keywords[:amount]
    logger.info(f"Final script-based Korean keywords: {result}")
    return result

def _get_korean_subject_keywords(video_subject: str) -> List[str]:
    """ì£¼ì œ ê¸°ë°˜ í•œêµ­ì–´ í‚¤ì›Œë“œ ìƒì„±"""
    subject_lower = video_subject.lower()
    
    subject_mapping = {
        "ì•„ì¹¨": ["ì•„ì¹¨ë£¨í‹´", "ëª¨ë‹", "í•˜ë£¨ì‹œì‘", "ê¸°ìƒ"],
        "ë£¨í‹´": ["ì¼ìƒë£¨í‹´", "ìƒí™œíŒ¨í„´", "ìŠµê´€", "ë¼ì´í”„ìŠ¤íƒ€ì¼"],
        "ìŠµê´€": ["ì¢‹ì€ìŠµê´€", "ìŠµê´€ë§Œë“¤ê¸°", "ìê¸°ê´€ë¦¬", "ë¼ì´í”„ìŠ¤íƒ€ì¼"],
        "ê±´ê°•": ["ê±´ê°•ê´€ë¦¬", "ì›°ë¹™", "ì²´ë ¥", "ê±´ê°•ë²•"],
        "ìš´ë™": ["ìš´ë™ë²•", "í—¬ìŠ¤", "ì²´ë ¥ê´€ë¦¬", "í”¼íŠ¸ë‹ˆìŠ¤"],
        "ë‹¤ì´ì–´íŠ¸": ["ë‹¤ì´ì–´íŠ¸", "ì²´ì¤‘ê´€ë¦¬", "ê±´ê°•ì‹", "ìš´ë™ë²•"],
        "ëˆ": ["ì¬í…Œí¬", "ëˆê´€ë¦¬", "ê²½ì œ", "íˆ¬ì"],
        "íˆ¬ì": ["íˆ¬ìë²•", "ì¬í…Œí¬", "ê²½ì œ", "ìì‚°ê´€ë¦¬"],
        "ì„±ê³µ": ["ì„±ê³µë²•", "ìê¸°ê³„ë°œ", "ëª©í‘œë‹¬ì„±", "ì„±ì¥"],
        "ìŠ¤íŠ¸ë ˆìŠ¤": ["ìŠ¤íŠ¸ë ˆìŠ¤ê´€ë¦¬", "ë§ˆìŒê´€ë¦¬", "íë§", "íœ´ì‹"],
        "ì‹œê°„": ["ì‹œê°„ê´€ë¦¬", "íš¨ìœ¨ì„±", "ìƒì‚°ì„±", "ê³„íš"],
        "ê´€ë¦¬": ["ìê¸°ê´€ë¦¬", "ìƒí™œê´€ë¦¬", "íš¨ìœ¨ì„±", "ì¡°ì ˆ"],
        "ë°©ë²•": ["ë…¸í•˜ìš°", "íŒ", "ë¹„ë²•", "í•´ê²°ì±…"],
        "ë§ˆìŒ": ["ë§ˆìŒê´€ë¦¬", "ì •ì‹ ê±´ê°•", "ê°ì •", "ì‹¬ë¦¬"]
    }
    
    keywords = []
    for korean, korean_list in subject_mapping.items():
        if korean in subject_lower:
            keywords.extend(korean_list)
    
    return keywords[:8] if keywords else ["ì •ë³´", "íŒ", "ë…¸í•˜ìš°", "ì¼ìƒ"]


def _remove_greetings(script: str, language: str = "auto") -> str:
    """ëŒ€ë³¸ì—ì„œ ì¸ì‚¬ë§ ì œê±°"""
    if not script:
        return script
    
    lines = script.split('\n')
    cleaned_lines = []
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # í•œêµ­ì–´ ì¸ì‚¬ë§ íŒ¨í„´
        korean_greetings = [
            "ì•ˆë…•í•˜ì„¸ìš”", "ì•ˆë…•", "ì—¬ëŸ¬ë¶„", "ì‹œì²­ì ì—¬ëŸ¬ë¶„", "êµ¬ë…ì ì—¬ëŸ¬ë¶„",
            "ì˜¤ëŠ˜ì€", "ì˜¤ëŠ˜ ì˜ìƒì—ì„œëŠ”", "ì´ë²ˆ ì˜ìƒì—ì„œëŠ”", "ë°˜ê°‘ìŠµë‹ˆë‹¤",
            "í™˜ì˜í•©ë‹ˆë‹¤", "ë‹¤ì‹œ ë§Œë‚˜ëµ™ìŠµë‹ˆë‹¤", "ì±„ë„ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤",
            "ì˜¤ëŠ˜ë„", "ë‹¤ì‹œ í•œë²ˆ"
        ]
        
        # ì˜ì–´ ì¸ì‚¬ë§ íŒ¨í„´
        english_greetings = [
            "hello", "hi everyone", "hi there", "welcome", "welcome back",
            "good morning", "good afternoon", "good evening", "hey guys",
            "what's up", "greetings", "welcome to", "hey there", "hi folks",
            "today we", "in today's video", "today i", "today's topic"
        ]
        
        # ì¸ì‚¬ë§ë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì¥ ì œê±°
        should_skip = False
        line_lower = line.lower()
        
        if language == "ko-KR" or language == "auto":
            for greeting in korean_greetings:
                if line.startswith(greeting) or greeting in line[:30]:
                    should_skip = True
                    break
        
        if language == "en-US" or not should_skip:
            for greeting in english_greetings:
                if (line_lower.startswith(greeting + " ") or 
                    line_lower.startswith(greeting + ",") or
                    line_lower.startswith(greeting + ".") or
                    line_lower == greeting):
                    should_skip = True
                    break
        
        # ì¸ì‚¬ë§ì´ ì•„ë‹Œ ë¬¸ì¥ë§Œ ì¶”ê°€
        if not should_skip:
            cleaned_lines.append(line)
    
    # ê²°ê³¼ ì¡°í•©
    result = '\n'.join(cleaned_lines).strip()
    
    # ë¹ˆ ê²°ê³¼ì¸ ê²½ìš° ì›ë³¸ ë°˜í™˜ (ëª¨ë“  ë¬¸ì¥ì´ ì¸ì‚¬ë§ì¸ ê²½ìš° ë°©ì§€)
    if not result or len(result) < 20:
        # ì²« ë²ˆì§¸ ë¬¸ì¥ë§Œ ì œê±°í•˜ê³  ë‚˜ë¨¸ì§€ ë°˜í™˜
        original_lines = script.split('\n')
        if len(original_lines) > 1:
            return '\n'.join(original_lines[1:]).strip()
        else:
            return script
    
    return result
def _clean_markdown_formatting(script: str) -> str:
    """ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì œê±°"""
    if not script:
        return script
    
    import re
    
    # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì œê±°
    # ** ë³¼ë“œ ì œê±°
    script = re.sub(r'\*\*(.*?)\*\*', r'\1', script)
    
    # * ì´íƒ¤ë¦­ ì œê±°
    script = re.sub(r'\*(.*?)\*', r'\1', script)
    
    # ## í—¤ë” ì œê±°
    script = re.sub(r'^#{1,6}\s*', '', script, flags=re.MULTILINE)
    
    # - ë¦¬ìŠ¤íŠ¸ ì œê±°
    script = re.sub(r'^-\s*', '', script, flags=re.MULTILINE)
    
    # 1. ìˆ«ì ë¦¬ìŠ¤íŠ¸ ì œê±°
    script = re.sub(r'^\d+\.\s*', '', script, flags=re.MULTILINE)
    
    # [ë§í¬](url) í˜•ì‹ ì œê±°
    script = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', script)
    
    # ``` ì½”ë“œ ë¸”ë¡ ì œê±°
    script = re.sub(r'```.*?```', '', script, flags=re.DOTALL)
    
    # ` ì¸ë¼ì¸ ì½”ë“œ ì œê±°
    script = re.sub(r'`([^`]+)`', r'\1', script)
    
    # > ì¸ìš©ë¬¸ ì œê±°
    script = re.sub(r'^>\s*', '', script, flags=re.MULTILINE)
    
    # [ì¥ë©´ 1], [ì¥ë©´ 2] ë“± ì¥ë©´ ì„¤ëª… ì œê±°
    script = re.sub(r'\[ì¥ë©´\s*\d+\]', '', script)
    script = re.sub(r'\[Scene\s*\d+\]', '', script)
    
    # ì—°ì†ëœ ê³µë°± ì •ë¦¬
    script = re.sub(r'\s+', ' ', script)
    
    # ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì •ë¦¬
    script = re.sub(r'\n\s*\n', '\n\n', script)
    
    return script.strip()
def _generate_enhanced_script_keywords(video_subject: str, video_script: str, amount: int = 5) -> List[str]:
    """Enhanced fallback that analyzes script content more deeply"""
    logger.info(f"Performing enhanced script content analysis for: '{video_subject}'")
    
    script_lower = video_script.lower()
    subject_lower = video_subject.lower()
    
    # Comprehensive Korean-to-English mapping with script context analysis
    content_mapping = {
        # Actions and behaviors
        "ë£¨í‹´": ["routine", "daily habits"],
        "ìŠµê´€": ["habits", "lifestyle"],
        "ë°©ë²•": ["method", "technique"],
        "ë¹„ë²•": ["secret", "strategy"],
        "íŒ": ["tips", "advice"],
        "ê°€ì´ë“œ": ["guide", "tutorial"],
        "ìš´ë™": ["workout", "exercise"],
        "ëª…ìƒ": ["meditation", "mindfulness"],
        "ì§‘ì¤‘": ["focus", "concentration"],
        "ê³µë¶€": ["study", "learning"],
        "ë…ì„œ": ["reading", "books"],
        "ìš”ë¦¬": ["cooking", "kitchen"],
        "ì²­ì†Œ": ["cleaning", "organization"],
        
        # Emotions and states
        "ì„±ê³µ": ["success", "achievement"],
        "í–‰ë³µ": ["happiness", "joy"],
        "ìŠ¤íŠ¸ë ˆìŠ¤": ["stress", "pressure"],
        "í”¼ë¡œ": ["tired", "exhausted"],
        "ì—ë„ˆì§€": ["energy", "vitality"],
        "ìì‹ ê°": ["confidence", "self-esteem"],
        "ë™ê¸°": ["motivation", "inspiration"],
        
        # Objects and tools
        "ëˆ": ["money", "cash"],
        "íˆ¬ì": ["investment", "finance"],
        "ë¶€ë™ì‚°": ["real estate", "property"],
        "ìë™ì°¨": ["car", "vehicle"],
        "í•¸ë“œí°": ["smartphone", "phone"],
        "ì»´í“¨í„°": ["computer", "laptop"],
        "ì±…": ["book", "reading"],
        "ìŒì‹": ["food", "meal"],
        "ì»¤í”¼": ["coffee", "cafe"],
        
        # Places and environments
        "ì§‘": ["home", "house"],
        "ì‚¬ë¬´ì‹¤": ["office", "workplace"],
        "ì¹´í˜": ["cafe", "coffee shop"],
        "í—¬ìŠ¤ì¥": ["gym", "fitness"],
        "ë„ì„œê´€": ["library", "study"],
        "ê³µì›": ["park", "outdoor"],
        "ë°”ë‹¤": ["ocean", "beach"],
        "ì‚°": ["mountain", "hiking"],
        
        # Time and scheduling
        "ì•„ì¹¨": ["morning", "sunrise"],
        "ì €ë…": ["evening", "sunset"],
        "ë°¤": ["night", "dark"],
        "ì£¼ë§": ["weekend", "leisure"],
        "íœ´ê°€": ["vacation", "holiday"],
        "ì‹œê°„": ["time", "clock"],
        
        # Health and wellness
        "ê±´ê°•": ["health", "wellness"],
        "ë‹¤ì´ì–´íŠ¸": ["diet", "weight loss"],
        "ì˜ì–‘": ["nutrition", "healthy food"],
        "ìˆ˜ë©´": ["sleep", "rest"],
        "íœ´ì‹": ["rest", "relaxation"],
        
        # Relationships and social
        "ê°€ì¡±": ["family", "together"],
        "ì¹œêµ¬": ["friends", "social"],
        "ì—°ì¸": ["couple", "romance"],
        "ê²°í˜¼": ["wedding", "marriage"],
        "ì•„ì´": ["children", "kids"],
        "ë¶€ëª¨": ["parents", "family"],
        
        # Work and career
        "ì¼": ["work", "business"],
        "íšŒì‚¬": ["company", "corporate"],
        "ì°½ì—…": ["startup", "entrepreneur"],
        "ë©´ì ‘": ["interview", "job"],
        "ìŠ¹ì§„": ["promotion", "career"],
        "í‡´ì‚¬": ["resignation", "quit job"],
        
        # Technology and modern life
        "AI": ["artificial intelligence", "technology"],
        "ë””ì§€í„¸": ["digital", "tech"],
        "ì˜¨ë¼ì¸": ["online", "internet"],
        "ì†Œì…œë¯¸ë””ì–´": ["social media", "smartphone"],
        "ìœ íŠœë¸Œ": ["content creation", "video"],
        "ì¸ìŠ¤íƒ€ê·¸ë¨": ["social media", "photography"]
    }
    
    # Analyze script content for matching keywords
    matched_keywords = []
    
    # First, look for direct matches in script content
    for korean_term, english_terms in content_mapping.items():
        if korean_term in script_lower:
            matched_keywords.extend(english_terms[:2])  # Take first 2 from each match
            logger.info(f"Found script content match: '{korean_term}' -> {english_terms[:2]}")
    
    # Then, look for subject-based matches
    for korean_term, english_terms in content_mapping.items():
        if korean_term in subject_lower and korean_term not in script_lower:
            matched_keywords.extend(english_terms[:1])  # Take 1 from subject matches
            logger.info(f"Found subject match: '{korean_term}' -> {english_terms[:1]}")
    
    # Remove duplicates while preserving order
    unique_keywords = []
    seen = set()
    for keyword in matched_keywords:
        if keyword.lower() not in seen:
            unique_keywords.append(keyword.lower())
            seen.add(keyword.lower())
    
    # If we don't have enough keywords, add contextual ones
    if len(unique_keywords) < amount:
        contextual_keywords = []
        
        # Add context-based keywords based on script tone and content
        if any(word in script_lower for word in ["ì„±ê³µ", "ë‹¬ì„±", "ëª©í‘œ", "ì´ë£¨"]):
            contextual_keywords.extend(["success", "achievement", "goal"])
        if any(word in script_lower for word in ["ê±´ê°•", "ìš´ë™", "ë‹¤ì´ì–´íŠ¸", "ëª¸"]):
            contextual_keywords.extend(["health", "fitness", "wellness"])
        if any(word in script_lower for word in ["ëˆ", "íˆ¬ì", "ë¶€ì", "ê²½ì œ"]):
            contextual_keywords.extend(["money", "finance", "wealth"])
        if any(word in script_lower for word in ["ì‹œê°„", "íš¨ìœ¨", "ìƒì‚°ì„±", "ê´€ë¦¬"]):
            contextual_keywords.extend(["time", "productivity", "efficiency"])
        if any(word in script_lower for word in ["í–‰ë³µ", "ë§Œì¡±", "ê¸°ì¨", "ì¦ê±°"]):
            contextual_keywords.extend(["happiness", "joy", "satisfaction"])
        
        # Add unique contextual keywords
        for keyword in contextual_keywords:
            if keyword not in seen and len(unique_keywords) < amount:
                unique_keywords.append(keyword)
                seen.add(keyword)
    
    # Final fallback with universal keywords if still not enough
    if len(unique_keywords) < amount:
        universal_keywords = ["lifestyle", "people", "modern", "daily life", "professional"]
        for keyword in universal_keywords:
            if keyword not in seen and len(unique_keywords) < amount:
                unique_keywords.append(keyword)
                seen.add(keyword)
    
    result = unique_keywords[:amount]
    logger.info(f"Enhanced script analysis final keywords: {result}")
    return result


def generate_longform_script(video_subject: str, language: str = "ko-KR", duration_minutes: int = 10) -> str:
    """ë¡±í¼ ì˜ìƒìš© ê¸´ ëŒ€ë³¸ ìƒì„± (5-15ë¶„ ë¶„ëŸ‰)"""
    logger.info(f"Generating long-form script for subject: {video_subject}, duration: {duration_minutes} minutes")
    
    # ì–¸ì–´ë³„ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    if language == "ko-KR" or language == "auto":
        prompt = f"""
        ì£¼ì œ '{video_subject}'ì— ëŒ€í•œ {duration_minutes}ë¶„ ë¶„ëŸ‰ì˜ ë¡±í¼ YouTube ì˜ìƒ ëŒ€ë³¸ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

        ìš”êµ¬ì‚¬í•­:
        1. ì´ {duration_minutes}ë¶„ ë¶„ëŸ‰ (ì•½ {duration_minutes * 150}ì)
        2. ì¸íŠ¸ë¡œ - ë³¸ë¡  - ì•„ì›ƒíŠ¸ë¡œ êµ¬ì¡°
        3. 5-7ê°œì˜ ì£¼ìš” ì±•í„°ë¡œ êµ¬ì„±
        4. ê° ì±•í„°ëŠ” ëª…í™•í•œ ì†Œì œëª©ê³¼ 2-3ë¶„ ë¶„ëŸ‰
        5. ì‹œì²­ì ì°¸ì—¬ ìœ ë„ ìš”ì†Œ í¬í•¨ (ì§ˆë¬¸, ëŒ“ê¸€ ìœ ë„ ë“±)
        6. êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì •ë³´ ì œê³µ
        7. ìì—°ìŠ¤ëŸ¬ìš´ ë§íˆ¬ì™€ íë¦„
        8. ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì‚¬ìš© ê¸ˆì§€
        9. ì¥ë©´ ì„¤ëª… ì‚¬ìš© ê¸ˆì§€
        10. ìˆœìˆ˜í•œ í…ìŠ¤íŠ¸ë§Œ ì‘ì„±

        êµ¬ì¡°:
        - ì¸íŠ¸ë¡œ: ì£¼ì œ ì†Œê°œ ë° ì‹œì²­ì ê´€ì‹¬ ëŒê¸°
        - ë³¸ë¡ : 5-7ê°œ ì±•í„°ë¡œ ë‚˜ëˆ„ì–´ ìƒì„¸ ì„¤ëª…
        - ì•„ì›ƒíŠ¸ë¡œ: ìš”ì•½ ë° êµ¬ë…/ì¢‹ì•„ìš” ìœ ë„

        ìŠ¤íƒ€ì¼: êµìœ¡ì ì´ë©´ì„œë„ í¥ë¯¸ë¡­ê²Œ, ì „ë¬¸ì ì´ì§€ë§Œ ì´í•´í•˜ê¸° ì‰½ê²Œ

        ì£¼ì œ: {video_subject}

        {duration_minutes}ë¶„ ë¶„ëŸ‰ì˜ ë¡±í¼ ëŒ€ë³¸ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
        """
    else:
        prompt = f"""
        Write a {duration_minutes}-minute long-form YouTube video script about '{video_subject}'.

        Requirements:
        1. Total duration: {duration_minutes} minutes (approximately {duration_minutes * 120} words)
        2. Structure: Intro - Main Content - Outro
        3. 5-7 main chapters
        4. Each chapter: clear subtitle and 2-3 minutes content
        5. Include viewer engagement elements (questions, comment prompts)
        6. Provide specific and practical information
        7. Natural speaking tone and flow
        8. NO markdown formatting
        9. NO scene descriptions
        10. Plain text only

        Structure:
        - Intro: Topic introduction and hook
        - Main: 5-7 chapters with detailed explanations
        - Outro: Summary and subscribe/like call-to-action

        Style: Educational yet engaging, professional but accessible

        Subject: {video_subject}

        Write the {duration_minutes}-minute long-form script:
        """
    
    try:
        response = _generate_response(prompt)
        if response:
            script = response.strip()
            
            # ì¸ì‚¬ë§ ì œê±° ë¡œì§
            script = _remove_greetings(script, language)
            
            # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì œê±°
            script = _clean_markdown_formatting(script)
            
            logger.info(f"Generated long-form script: {len(script)} characters")
            return script
    except Exception as e:
        logger.error(f"Failed to generate long-form script: {e}")
    
    return "ë¡±í¼ ëŒ€ë³¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."


def split_longform_script(script: str, segment_duration: int = 3) -> list:
    """ë¡±í¼ ëŒ€ë³¸ì„ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë¶„í• """
    logger.info(f"Splitting long-form script into {segment_duration}-minute segments")
    
    # ëŒ€ë³¸ì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
    sentences = script.replace('\n\n', '\n').split('\n')
    sentences = [s.strip() for s in sentences if s.strip()]
    
    # ê° ì„¸ê·¸ë¨¼íŠ¸ë‹¹ ì˜ˆìƒ ë¬¸ì ìˆ˜ (3ë¶„ = ì•½ 450ì)
    chars_per_segment = segment_duration * 150
    
    segments = []
    current_segment = ""
    
    for sentence in sentences:
        # í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ì— ë¬¸ì¥ì„ ì¶”ê°€í–ˆì„ ë•Œì˜ ê¸¸ì´ í™•ì¸
        test_segment = current_segment + "\n" + sentence if current_segment else sentence
        
        if len(test_segment) <= chars_per_segment:
            current_segment = test_segment
        else:
            # í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ë” ì¶”ê°€
            if len(current_segment) < chars_per_segment * 0.7:
                current_segment = test_segment
            else:
                # ì„¸ê·¸ë¨¼íŠ¸ ì™„ì„±
                if current_segment:
                    segments.append(current_segment.strip())
                current_segment = sentence
    
    # ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ê°€
    if current_segment:
        segments.append(current_segment.strip())
    
    logger.info(f"Split script into {len(segments)} segments")
    return segments


def generate_longform_background_keywords(video_subject: str, script_segment: str, segment_index: int) -> list:
    """ë¡±í¼ ì˜ìƒ ì„¸ê·¸ë¨¼íŠ¸ë³„ ë°°ê²½ ì˜ìƒ í‚¤ì›Œë“œ ìƒì„± - í–¥ìƒëœ ëŒ€ë³¸ ë¶„ì„ ê¸°ë°˜"""
    logger.info(f"Generating enhanced background keywords for longform segment {segment_index}")
    
    # í–¥ìƒëœ ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„ í”„ë¡¬í”„íŠ¸
    prompt = f"""
    You are a professional video editor specializing in matching longform content with perfect background footage.

    TASK: Analyze this specific segment of a longform video and generate HIGHLY SPECIFIC English keywords for background video search.

    LONGFORM CONTEXT:
    - Overall Subject: {video_subject}
    - Segment Number: {segment_index}
    - Segment Content: {script_segment}

    ENHANCED ANALYSIS REQUIREMENTS:
    1. Read the segment content sentence by sentence
    2. Identify SPECIFIC visual elements, actions, and concepts mentioned
    3. Convert to English keywords that will find EXACTLY matching stock footage
    4. Focus on CONCRETE, FILMABLE elements (not abstract concepts)
    5. Each keyword should be 1-2 words maximum for better search results
    6. Ensure variety - different segments should have different visual styles
    7. Consider the educational/informational nature of longform content

    LONGFORM-SPECIFIC CONSIDERATIONS:
    - Longer content needs more visual variety
    - Educational content benefits from professional, clean visuals
    - Each segment should have distinct visual identity
    - Mix of close-ups, wide shots, and different environments

    EXAMPLES OF EXCELLENT LONGFORM KEYWORD MATCHING:
    - Segment about "ì‹œê°„ ê´€ë¦¬" â†’ "time management", "clock", "schedule", "planning"
    - Segment about "ì„±ê³µ ìŠµê´€" â†’ "business success", "professional", "achievement", "goal"
    - Segment about "ê±´ê°•í•œ ì‹ìŠµê´€" â†’ "healthy food", "nutrition", "cooking", "fresh vegetables"
    - Segment about "ìš´ë™ ë£¨í‹´" â†’ "workout", "gym", "exercise", "fitness training"

    Based on the segment analysis above, generate 8 HIGHLY SPECIFIC English keywords that will find footage perfectly matching this segment's content.
    
    Return ONLY the keywords separated by commas, no explanations:
    """
    
    try:
        logger.info(f"Generating segment-specific keywords using enhanced LLM analysis for segment {segment_index}...")
        response = _generate_response(prompt)
        logger.info(f"LLM response for segment {segment_index}: {response}")
        
        if response:
            # Clean up the response
            cleaned = response.strip()
            
            # Remove common prefixes
            prefixes = ["keywords:", "Keywords:", "Sure", "The keywords", "Based on", "For this", "Here are", "Analysis:", "Result:"]
            for p in prefixes:
                if cleaned.lower().startswith(p.lower()):
                    cleaned = cleaned[len(p):].strip()
                    if cleaned.startswith(":"):
                        cleaned = cleaned[1:].strip()
            
            # Clean formatting and extract terms
            cleaned = cleaned.replace("\n", ",").replace("- ", "").replace("* ", "").replace('"', '').replace("'", "").strip()
            keywords = [k.strip() for k in cleaned.split(",") if k.strip()]
            
            # Enhanced validation for longform content
            valid_keywords = []
            for keyword in keywords:
                keyword = keyword.strip().lower()
                # Strict validation for longform content matching
                if (len(keyword.split()) <= 2 and  # Max 2 words
                    keyword.isascii() and 
                    len(keyword) > 2 and
                    # Exclude generic/meta terms
                    not any(generic in keyword for generic in [
                        "ai", "video", "content", "longform", "segment", "analysis", 
                        "keyword", "example", "concept", "youtube", "education"
                    ]) and
                    # Exclude common stop words
                    not any(stop_word in keyword for stop_word in [
                        "the", "and", "or", "but", "with", "for", "this", "that"
                    ]) and
                    # Ensure it's a concrete, searchable term
                    not keyword.startswith(("how to", "what is", "why", "when"))):
                    valid_keywords.append(keyword)
            
            logger.info(f"Segment {segment_index} valid keywords: {valid_keywords}")
            
            if len(valid_keywords) >= 5:
                final_keywords = valid_keywords[:8]
                logger.info(f"Final longform segment {segment_index} keywords: {final_keywords}")
                return final_keywords
                
    except Exception as e:
        logger.error(f"Failed to generate longform segment keywords: {e}")
    
    # Enhanced fallback with segment-specific analysis
    logger.warning(f"LLM failed for segment {segment_index}. Using enhanced segment analysis fallback...")
    return _generate_longform_segment_keywords(video_subject, script_segment, segment_index)


def _generate_longform_segment_keywords(video_subject: str, script_segment: str, segment_index: int) -> list:
    """Enhanced fallback for longform segment keyword generation"""
    logger.info(f"Performing enhanced longform segment analysis for segment {segment_index}")
    
    script_lower = script_segment.lower()
    subject_lower = video_subject.lower()
    
    # Longform-specific keyword mapping with more professional/educational focus
    longform_mapping = {
        # Professional and business
        "ì„±ê³µ": ["business success", "achievement", "professional", "goal"],
        "ë¹„ì¦ˆë‹ˆìŠ¤": ["business", "corporate", "office", "meeting"],
        "ì „ëµ": ["strategy", "planning", "business plan", "analysis"],
        "ëª©í‘œ": ["goal", "target", "achievement", "success"],
        "ê³„íš": ["planning", "strategy", "organization", "schedule"],
        
        # Learning and education
        "í•™ìŠµ": ["learning", "education", "study", "knowledge"],
        "êµìœ¡": ["education", "teaching", "classroom", "training"],
        "ì§€ì‹": ["knowledge", "information", "learning", "education"],
        "ì—°êµ¬": ["research", "analysis", "study", "investigation"],
        "ë¶„ì„": ["analysis", "data", "research", "statistics"],
        
        # Health and wellness (professional focus)
        "ê±´ê°•": ["health", "wellness", "medical", "healthcare"],
        "ìš´ë™": ["exercise", "fitness", "gym", "training"],
        "ì˜ì–‘": ["nutrition", "healthy food", "diet", "wellness"],
        "ì˜ë£Œ": ["medical", "healthcare", "doctor", "hospital"],
        
        # Technology and innovation
        "ê¸°ìˆ ": ["technology", "innovation", "digital", "tech"],
        "í˜ì‹ ": ["innovation", "technology", "modern", "future"],
        "ë””ì§€í„¸": ["digital", "technology", "computer", "online"],
        "AI": ["artificial intelligence", "technology", "computer", "digital"],
        
        # Finance and economics
        "ê²½ì œ": ["economy", "finance", "business", "market"],
        "íˆ¬ì": ["investment", "finance", "money", "business"],
        "ê¸ˆìœµ": ["finance", "banking", "investment", "money"],
        "ì‹œì¥": ["market", "business", "economy", "trading"],
        
        # Time and productivity
        "ì‹œê°„": ["time", "clock", "productivity", "schedule"],
        "íš¨ìœ¨": ["efficiency", "productivity", "optimization", "performance"],
        "ìƒì‚°ì„±": ["productivity", "efficiency", "work", "performance"],
        "ê´€ë¦¬": ["management", "organization", "control", "system"],
        
        # Communication and social
        "ì†Œí†µ": ["communication", "meeting", "discussion", "teamwork"],
        "í˜‘ì—…": ["teamwork", "collaboration", "meeting", "group"],
        "ë¦¬ë”ì‹­": ["leadership", "management", "team", "professional"],
        "ë„¤íŠ¸ì›Œí‚¹": ["networking", "business", "professional", "meeting"],
        
        # Environment and sustainability
        "í™˜ê²½": ["environment", "nature", "sustainability", "green"],
        "ì§€ì†ê°€ëŠ¥": ["sustainability", "environment", "green", "eco"],
        "ìì—°": ["nature", "environment", "outdoor", "landscape"],
        
        # Psychology and mindset
        "ì‹¬ë¦¬": ["psychology", "mindset", "mental", "brain"],
        "ë§ˆìŒ": ["mindset", "psychology", "mental", "emotion"],
        "ê°ì •": ["emotion", "psychology", "mental", "feeling"],
        "ìŠ¤íŠ¸ë ˆìŠ¤": ["stress", "pressure", "mental health", "wellness"]
    }
    
    # Analyze segment content for matching keywords
    matched_keywords = []
    
    # Look for direct matches in segment content (prioritized)
    for korean_term, english_terms in longform_mapping.items():
        if korean_term in script_lower:
            matched_keywords.extend(english_terms[:2])  # Take 2 from each match
            logger.info(f"Segment {segment_index} content match: '{korean_term}' -> {english_terms[:2]}")
    
    # Look for subject-based matches (secondary)
    for korean_term, english_terms in longform_mapping.items():
        if korean_term in subject_lower and korean_term not in script_lower:
            matched_keywords.extend(english_terms[:1])  # Take 1 from subject matches
            logger.info(f"Segment {segment_index} subject match: '{korean_term}' -> {english_terms[:1]}")
    
    # Remove duplicates while preserving order
    unique_keywords = []
    seen = set()
    for keyword in matched_keywords:
        if keyword.lower() not in seen:
            unique_keywords.append(keyword.lower())
            seen.add(keyword.lower())
    
    # Add segment-specific variety based on index
    if len(unique_keywords) < 8:
        # Different visual styles for different segments
        segment_variety = {
            1: ["professional", "business", "modern"],
            2: ["technology", "innovation", "digital"],
            3: ["analysis", "data", "research"],
            4: ["teamwork", "collaboration", "meeting"],
            5: ["growth", "development", "progress"],
            6: ["strategy", "planning", "organization"],
            7: ["success", "achievement", "goal"],
            8: ["future", "vision", "innovation"]
        }
        
        variety_keywords = segment_variety.get(segment_index % 8 + 1, ["professional", "modern", "business"])
        for keyword in variety_keywords:
            if keyword not in seen and len(unique_keywords) < 8:
                unique_keywords.append(keyword)
                seen.add(keyword)
    
    # Final fallback with longform-appropriate keywords
    if len(unique_keywords) < 8:
        longform_fallback = [
            "professional", "business", "modern", "technology", 
            "education", "analysis", "strategy", "innovation"
        ]
        for keyword in longform_fallback:
            if keyword not in seen and len(unique_keywords) < 8:
                unique_keywords.append(keyword)
                seen.add(keyword)
    
    result = unique_keywords[:8]
    logger.info(f"Enhanced longform segment {segment_index} final keywords: {result}")
    return result