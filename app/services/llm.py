import json
import logging
import re
from collections import Counter
from typing import List, Optional

from loguru import logger
from openai import AzureOpenAI, OpenAI

from app.config import config

_max_retries = 3

def _generate_response(prompt: str) -> str:
    config.reload()
    
    content = ""
    llm_provider = config.app.get("llm_provider", "gemini")
    logger.info(f"llm provider: {llm_provider}")
    
    # Multi-API key rotation for better quota management
    if llm_provider == "gemini":
        # Collect all available Gemini API keys (up to 5 keys)
        gemini_keys = []
        for i in range(1, 6):  # gemini_api_key, gemini_api_key_2, ..., gemini_api_key_5
            if i == 1:
                key = config.app.get("gemini_api_key")
            else:
                key = config.app.get(f"gemini_api_key_{i}")
            if key:
                gemini_keys.append((i, key))
        
        if not gemini_keys:
            logger.error("No Gemini API keys configured")
            raise Exception("No Gemini API keys available")
        
        logger.info(f"Found {len(gemini_keys)} Gemini API keys for rotation")
        
        # Try each key until one works
        for key_num, api_key in gemini_keys:
            try:
                logger.info(f"Trying Gemini API key #{key_num} ({len(gemini_keys)} total)")
                import google.generativeai as genai
                genai.configure(api_key=api_key)
                
                model_name = config.app.get("gemini_model_name", "gemini-2.5-flash")
                target_model = model_name.strip()
                models_to_try = [
                    target_model,
                    "gemini-2.5-flash",
                    "gemini-flash-latest",
                    "gemini-2.0-flash",
                    "gemini-2.0-flash-exp",
                ]
                
                for m in models_to_try:
                    try:
                        logger.info(f"Using Gemini model: {m} with API key #{key_num}")
                        model = genai.GenerativeModel(m)
                        response = model.generate_content(prompt)
                        if response and getattr(response, "text", None):
                            logger.success(f"âœ… Success with API key #{key_num} and model {m}")
                            return response.text
                    except Exception as e_try:
                        logger.warning(f"Gemini model {m} failed with key #{key_num}: {e_try}")
                        if "429" in str(e_try) or "Quota exceeded" in str(e_try) or "Resource has been exhausted" in str(e_try) or "RESOURCE_EXHAUSTED" in str(e_try):
                            logger.warning(f"ğŸš« API key #{key_num} quota exceeded, trying next key...")
                            break
                        continue
                        
            except Exception as e:
                logger.warning(f"âŒ Gemini API key #{key_num} failed: {e}")
                if "429" in str(e) or "Quota exceeded" in str(e) or "Resource has been exhausted" in str(e) or "RESOURCE_EXHAUSTED" in str(e):
                    logger.warning(f"ğŸš« API key #{key_num} quota exceeded")
                continue
        
        # If all Gemini keys fail, fall back to DeepSeek
        logger.error("âŒ All Gemini API keys failed or quota exceeded, falling back to DeepSeek")
        llm_provider = "deepseek"
    
    # Remove the redirect to gemini for g4f
    if llm_provider in ["pollinations", "free"]:
        llm_provider = "gemini"

    # Standard Providers
    try:
        api_key = config.app.get(f"{llm_provider}_api_key")
        base_url = config.app.get(f"{llm_provider}_base_url")
        model_name = config.app.get(f"{llm_provider}_model_name")
        
        if llm_provider == "openai":
            client = OpenAI(api_key=api_key, base_url=base_url)
            response = client.chat.completions.create(
                model=model_name or "gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}]
            )
            return response.choices[0].message.content
            
        elif llm_provider == "moonshot":
            client = OpenAI(api_key=api_key, base_url="https://api.moonshot.cn/v1")
            response = client.chat.completions.create(
                model=model_name or "moonshot-v1-8k",
                messages=[{"role": "user", "content": prompt}]
            )
            return response.choices[0].message.content
            
        elif llm_provider == "azure":
            client = AzureOpenAI(
                api_key=api_key,
                api_version="2024-02-15-preview",
                azure_endpoint=base_url
            )
            response = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.choices[0].message.content
            
        elif llm_provider == "gemini":
            try:
                import google.generativeai as genai
                genai.configure(api_key=api_key)
                
                target_model = (model_name or "gemini-2.5-flash").strip()
                models_to_try = [
                    target_model,
                    "gemini-2.5-flash",
                    "gemini-flash-latest",
                    "gemini-2.0-flash",
                    "gemini-2.0-flash-exp",
                ]
                
                last_error = None
                for m in models_to_try:
                    try:
                        logger.info(f"Using Gemini model: {m}")
                        model = genai.GenerativeModel(m)
                        response = model.generate_content(prompt)
                        if response and getattr(response, "text", None):
                            return response.text
                    except Exception as e_try:
                        last_error = e_try
                        logger.warning(f"Gemini model {m} failed: {e_try}")
                        if "429" in str(e_try) or "Quota exceeded" in str(e_try):
                            logger.warning("Gemini Quota Exceeded. Skipping other models.")
                            break
                        continue
                
                try:
                    logger.info("Listing available Gemini models to auto-correct...")
                    available = []
                    for m in genai.list_models():
                        if 'generateContent' in m.supported_generation_methods:
                            available.append(m.name)
                    logger.info(f"Available models: {', '.join(available[:10])}...")
                except Exception as e_list:
                    logger.warning(f"Failed to list Gemini models: {e_list}")
                
                if last_error:
                    raise last_error
            except Exception as e:
                logger.warning(f"Gemini failed ({e})")
                raise e

        # Add other providers as needed (DeepSeek, Qwen, etc usually generic OpenAI)
        elif llm_provider in ["deepseek", "qwen", "ollama", "oneapi", "cloudflare", "ernie", "modelscope"]:
             # Generic OpenAI client
             if not base_url and llm_provider == "ollama":
                 base_url = "http://localhost:11434/v1"
             
             client = OpenAI(api_key=api_key or "dummy", base_url=base_url)
             response = client.chat.completions.create(
                model=model_name or "gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}]
            )
             return response.choices[0].message.content
             
        elif llm_provider == "g4f":
            try:
                import g4f
                logger.info("Using G4F provider")
                
                # Try multiple providers in order of reliability
                providers_to_try = [
                    g4f.Provider.DuckDuckGo,
                    g4f.Provider.Blackbox,
                    g4f.Provider.Airforce,
                    g4f.Provider.DarkAI,
                ]
                
                for provider in providers_to_try:
                    try:
                        logger.info(f"Trying G4F provider: {provider.__name__}")
                        response = g4f.ChatCompletion.create(
                            model="gpt-3.5-turbo",
                            provider=provider,
                            messages=[{"role": "user", "content": prompt}]
                        )
                        if response and response.strip():
                            logger.info(f"G4F success with {provider.__name__}")
                            return response.strip()
                    except Exception as e:
                        logger.warning(f"G4F provider {provider.__name__} failed: {e}")
                        continue
                
                # If all providers fail, raise the last error
                raise Exception("All G4F providers failed")
                
            except ImportError:
                logger.error("G4F not installed. Install with: pip install g4f")
                raise Exception("G4F not installed")
            except Exception as e:
                logger.error(f"G4F error: {e}")
                raise e

    except Exception as e:
        logger.error(f"LLM {llm_provider} error: {e}")
        raise e
        
    return ""

def generate_script(video_subject: str, language: str = "auto", paragraph_number: int = 1) -> str:
    # ì–¸ì–´ë³„ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    if language == "ko-KR" or language == "auto":
        prompt = f"""
        ì£¼ì œ '{video_subject}'ì— ëŒ€í•œ ìœ íŠœë¸Œ ì‡¼ì¸ ìš© ëŒ€ë³¸ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

        ìš”êµ¬ì‚¬í•­:
        1. {paragraph_number}ê°œì˜ ë¬¸ë‹¨ìœ¼ë¡œ êµ¬ì„±
        2. ê° ë¬¸ë‹¨ì€ 2-3ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±
        3. ì´ ê¸¸ì´ëŠ” 60-90ì´ˆ ë¶„ëŸ‰ (ì•½ 150-200ì)
        4. ì¸ì‚¬ë§(ì•ˆë…•í•˜ì„¸ìš”, ì—¬ëŸ¬ë¶„, ì‹œì²­ì ì—¬ëŸ¬ë¶„ ë“±) ì‚¬ìš© ê¸ˆì§€
        5. ë°”ë¡œ ë³¸ë¡ ìœ¼ë¡œ ì‹œì‘
        6. êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ë‚´ìš©
        7. ê°ì •ì ì´ê³  ë§¤ë ¥ì ì¸ í‘œí˜„ ì‚¬ìš©
        8. ì‹œì²­ìì˜ ê´€ì‹¬ì„ ëŒ ìˆ˜ ìˆëŠ” ë‚´ìš©
        9. ë§ˆí¬ë‹¤ìš´ í˜•ì‹(**, ##, - ë“±) ì‚¬ìš© ê¸ˆì§€
        10. ì¥ë©´ ì„¤ëª…([ì¥ë©´ 1] ë“±) ì‚¬ìš© ê¸ˆì§€
        11. ìˆœìˆ˜í•œ í…ìŠ¤íŠ¸ë§Œ ì‘ì„±

        ìŠ¤íƒ€ì¼: ì§ì ‘ì ì´ê³  ì„íŒ©íŠ¸ ìˆê²Œ, ì¸ì‚¬ë§ ì—†ì´ ë°”ë¡œ í•µì‹¬ ë‚´ìš©ìœ¼ë¡œ ì‹œì‘

        ì£¼ì œ: {video_subject}

        ëŒ€ë³¸ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
        """
    else:
        prompt = f"""
        Write a YouTube Shorts script about '{video_subject}'.

        Requirements:
        1. {paragraph_number} paragraphs
        2. Each paragraph: 2-3 sentences
        3. Total length: 60-90 seconds (about 150-200 words)
        4. NO greetings (Hello, Hi everyone, Welcome, etc.)
        5. Start directly with the main content
        6. Specific and practical information
        7. Engaging and emotional language
        8. Content that captures viewer attention
        9. NO markdown formatting (**, ##, - etc.)
        10. NO scene descriptions ([Scene 1] etc.)
        11. Plain text only

        Style: Direct and impactful, start immediately with core content

        Subject: {video_subject}

        Write the script:
        """
    
    # Check if subject is empty
    if not video_subject:
        return ""

    final_script = ""
    for i in range(_max_retries):
        try:
            response = _generate_response(prompt)
            if response:
                script = response.strip()
                
                # ì¸ì‚¬ë§ ì œê±° ë¡œì§
                script = _remove_greetings(script, language)
                
                # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì œê±°
                script = _clean_markdown_formatting(script)
                
                final_script = script
                break
        except Exception as e:
            logger.error(f"failed to generate script: {e}")
            
    if not final_script:
        return "AI ëŒ€ë³¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
        
    return final_script

def generate_terms(video_subject: str, video_script: str, amount: int = 5) -> List[str]:
    logger.info(f"Starting enhanced English keyword generation for subject: {video_subject}")
    
    # ëŒ€ë³¸ ë‚´ìš© ë¶„ì„ ê¸°ë°˜ ì˜ì–´ í‚¤ì›Œë“œ ìƒì„± í”„ë¡¬í”„íŠ¸
    prompt = f"""
    You are a professional video editor. Analyze the following script content to generate English keywords optimized for stock video search.

    Script Analysis & Keyword Generation Requirements:
    1. Convert specific actions, objects, and scenes mentioned in the script into English keywords
    2. Focus on visually filmable elements
    3. Each keyword should be 1-3 words
    4. Prioritize keywords directly related to the script's core message
    5. Use common terms easily found in stock footage
    6. DO NOT use generic terms like "AI generated", "viral", "content", "shorts", "video"
    7. Focus on script-mentioned actions, emotions, results, and methods

    Script Content Analysis:
    Subject: {video_subject}
    Script: {video_script[:1000]}

    Visual elements extractable from script:
    - Mentioned actions or movements
    - Objects or tools that appear
    - Described places or environments
    - Expressed emotions or states
    - Presented results or effects

    Based on the script content above, generate {amount} English keywords.
    List only the keywords separated by commas:
    """
    
    try:
        logger.info("Attempting to generate script-based English keywords using LLM...")
        response = _generate_response(prompt)
        logger.info(f"LLM response: {response}")
        
        if response:
            # Clean up the response more aggressively
            cleaned = response.strip()
            
            # Remove common prefixes
            prefixes = ["keywords:", "Keywords:", "Sure", "The keywords", "Based on", "For this", "Here are"]
            for p in prefixes:
                if cleaned.lower().startswith(p.lower()):
                    cleaned = cleaned[len(p):].strip()
                    if cleaned.startswith(":"):
                        cleaned = cleaned[1:].strip()
            
            # Clean formatting
            cleaned = cleaned.replace("\n", ",").replace("- ", "").replace("* ", "").replace('"', '').strip()
            logger.info(f"Cleaned response: {cleaned}")
            
            # Extract terms
            terms = [t.strip() for t in cleaned.split(",") if t.strip()]
            logger.info(f"Extracted terms: {terms}")
            
            # Filter and validate terms
            valid_terms = []
            for term in terms:
                term = term.strip().lower()
                # Skip if too long or contains non-English characters
                if (len(term.split()) <= 3 and term.isascii() and len(term) > 2 and
                    # Exclude generic terms
                    not any(generic in term for generic in ["ai generated", "viral", "content", "shorts", "video", "youtube"])):
                    # Skip common stop words
                    if not any(stop_word in term for stop_word in ["the", "and", "or", "but", "with", "for"]):
                        valid_terms.append(term)
            
            logger.info(f"Valid terms after filtering: {valid_terms}")
            
            if len(valid_terms) >= 3:
                logger.info(f"Generated script-based English keywords: {valid_terms[:amount]}")
                return valid_terms[:amount]
                
    except Exception as e:
        logger.error(f"failed to generate script-based English terms: {e}")
    
    # Enhanced fallback with script content analysis
    logger.warning("LLM failed to generate English terms. Using enhanced script analysis fallback.")
    return _generate_script_based_keywords(video_subject, video_script, amount)

def _generate_fallback_keywords(video_subject: str, video_script: str, amount: int = 5) -> List[str]:
    """Generate fallback keywords when LLM fails"""
    logger.info(f"Generating fallback keywords for subject: '{video_subject}' and script length: {len(video_script)}")
    
    fallback_keywords = []
    
    # Subject-based keywords
    subject_lower = video_subject.lower()
    script_lower = video_script.lower()
    
    # Enhanced Korean to English keyword mapping
    subject_keywords = {
        "ì„±ê³µ": ["success", "achievement", "business", "winner", "celebration"],
        "ê±´ê°•": ["health", "fitness", "wellness", "exercise", "nutrition"],
        "ëˆ": ["money", "finance", "wealth", "investment", "cash"],
        "íˆ¬ì": ["investment", "trading", "finance", "stock market", "business"],
        "ë‹¤ì´ì–´íŠ¸": ["diet", "weight loss", "fitness", "healthy eating", "workout"],
        "ìš´ë™": ["exercise", "workout", "fitness", "gym", "sports"],
        "ë…ì„œ": ["reading", "books", "education", "learning", "study"],
        "ê³µë¶€": ["study", "learning", "education", "school", "knowledge"],
        "ìŒì‹": ["food", "cooking", "nutrition", "kitchen", "meal"],
        "ì—¬í–‰": ["travel", "vacation", "adventure", "tourism", "journey"],
        "ì‚¬ë‘": ["love", "relationship", "romance", "couple", "heart"],
        "ê°€ì¡±": ["family", "home", "togetherness", "children", "parents"],
        "ì¼": ["work", "office", "career", "job", "business"],
        "ìŠ¤íŠ¸ë ˆìŠ¤": ["stress", "relaxation", "meditation", "calm", "peace"],
        "í–‰ë³µ": ["happiness", "joy", "celebration", "smile", "positive"],
        "ì‹œê°„": ["time", "clock", "schedule", "planning", "productivity"],
        "ìì—°": ["nature", "landscape", "outdoor", "forest", "mountains"],
        "ê¸°ìˆ ": ["technology", "innovation", "digital", "computer", "modern"],
        "ìŒì•…": ["music", "sound", "audio", "instruments", "melody"],
        "ì˜í™”": ["movie", "cinema", "entertainment", "film", "video"],
        "ê²Œì„": ["gaming", "play", "entertainment", "fun", "competition"],
        "ìš”ë¦¬": ["cooking", "kitchen", "food", "chef", "recipe"],
        "íŒ¨ì…˜": ["fashion", "style", "clothing", "design", "trendy"],
        "ë·°í‹°": ["beauty", "skincare", "cosmetics", "makeup", "care"],
        "êµìœ¡": ["education", "learning", "teaching", "school", "knowledge"],
        "ì°½ì—…": ["startup", "business", "entrepreneur", "innovation", "success"],
        "ë¶€ë™ì‚°": ["real estate", "property", "investment", "house", "building"],
        "ìë™ì°¨": ["car", "automotive", "driving", "vehicle", "transportation"],
        "ë°˜ë ¤ë™ë¬¼": ["pet", "animal", "dog", "cat", "care"],
        "ìœ¡ì•„": ["parenting", "children", "family", "baby", "care"],
        "ê²°í˜¼": ["wedding", "marriage", "couple", "love", "ceremony"],
        "ì·¨ì—…": ["job", "career", "employment", "work", "interview"]
    }
    
    # Find matching keywords from subject
    for korean, english_list in subject_keywords.items():
        if korean in subject_lower:
            fallback_keywords.extend(english_list[:3])  # Take first 3 from each match
            logger.info(f"Found subject keyword '{korean}' -> {english_list[:3]}")
    
    # Find matching keywords from script
    for korean, english_list in subject_keywords.items():
        if korean in script_lower and korean not in subject_lower:  # Avoid duplicates
            fallback_keywords.extend(english_list[:2])  # Take fewer from script
            logger.info(f"Found script keyword '{korean}' -> {english_list[:2]}")
    
    # Add generic visual keywords based on common themes
    if not fallback_keywords:
        logger.info("No specific keywords found, using generic keywords")
        fallback_keywords = ["lifestyle", "people", "modern", "business", "success"]
    
    # Add some universal keywords that work well for stock footage
    universal_keywords = ["lifestyle", "modern", "people", "business", "professional"]
    for keyword in universal_keywords:
        if keyword not in fallback_keywords:
            fallback_keywords.append(keyword)
    
    # Remove duplicates and limit
    unique_keywords = []
    seen = set()
    for keyword in fallback_keywords:
        if keyword.lower() not in seen:
            unique_keywords.append(keyword.lower())
            seen.add(keyword.lower())
    
    # Ensure we have enough keywords
    if len(unique_keywords) < amount:
        additional_keywords = ["creative", "inspiration", "motivation", "growth", "innovation"]
        for keyword in additional_keywords:
            if keyword not in seen and len(unique_keywords) < amount:
                unique_keywords.append(keyword)
                seen.add(keyword)
    
    result = unique_keywords[:amount]
    logger.info(f"Final fallback keywords: {result}")
    return result

def generate_english_script(video_subject: str, paragraph_number: int = 4) -> str:
    """Generate English script directly (for when translation fails)"""
    logger.info(f"Generating English script for subject: {video_subject}")
    
    prompt = f"""
    Create an engaging English script for a short video (60-90 seconds) about: {video_subject}

    REQUIREMENTS:
    1. Write {paragraph_number} paragraphs
    2. Each paragraph should be 2-3 sentences
    3. Use simple, clear English suitable for global audience
    4. Make it engaging and motivational
    5. Focus on practical tips or insights
    6. Use active voice and conversational tone
    7. No special formatting, just plain text

    STYLE: Motivational, educational, accessible to international viewers
    
    Subject: {video_subject}
    
    Write the script now:
    """
    
    try:
        response = _generate_response(prompt)
        if response and len(response.strip()) > 50:
            # Clean up the response
            script = response.strip()
            
            # Remove common prefixes
            prefixes = ["Here's", "Here is", "Script:", "The script", "Sure", "Certainly"]
            for prefix in prefixes:
                if script.startswith(prefix):
                    script = script[len(prefix):].strip()
                    if script.startswith(":"):
                        script = script[1:].strip()
            
            logger.info(f"Generated English script: {script[:100]}...")
            return script
            
    except Exception as e:
        logger.error(f"Failed to generate English script: {e}")
    
    # Fallback English script template
    logger.warning("Using fallback English script template")
    fallback_script = f"""
    Today we're exploring {video_subject}. This topic can transform your daily life in meaningful ways.

    Many successful people have discovered the power of focusing on what truly matters. Small changes in your routine can lead to remarkable results over time.

    The key is to start with simple, actionable steps. Don't try to change everything at once. Instead, pick one area and commit to consistent improvement.

    Remember, every expert was once a beginner. Your journey toward {video_subject} starts with a single decision to take action today.
    """
    
    return fallback_script.strip()
    
def translate_to_english(text: str) -> str:
    if not text:
        return ""
    
    # ì´ë¯¸ ì˜ì–´ì¸ì§€ í™•ì¸ (í•œê¸€ì´ ì—†ìœ¼ë©´ ì˜ì–´ë¡œ ê°„ì£¼)
    import re
    if not re.search(r'[ê°€-í£]', text):
        return text
    
    prompt = f"Translate the following Korean text into natural English. Return ONLY the translated text without any quotes, notes, or explanations:\n\n{text}"
    
    try:
        # 1ì°¨ ì‹œë„: ë©”ì¸ LLM ì‚¬ìš©
        response = _generate_response(prompt)
        if response and response.strip() != text and not re.search(r'[ê°€-í£]', response):
            logger.info(f"Successfully translated using main LLM: '{text}' -> '{response.strip()}'")
            return response.strip()
    except Exception as e:
        logger.warning(f"Main LLM failed for translation: {e}")
    
    logger.info("Main LLM failed for translation, trying free provider fallback...")
    
    try:
        # 2ì°¨ ì‹œë„: ë¬´ë£Œ ì œê³µì ì‚¬ìš©
        response = _generate_free_response(prompt)
        if response and response.strip() != text and not re.search(r'[ê°€-í£]', response):
            logger.info(f"Successfully translated using free provider: '{text}' -> '{response.strip()}'")
            return response.strip()
    except Exception as e:
        logger.warning(f"Free provider also failed: {e}")
    
    # 3ì°¨ ì‹œë„: ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤í•‘ (ë°±ì—…ìš©)
    try:
        simple_translations = {
            "ì„±ê³µ": "success",
            "ìŠµê´€": "habits", 
            "ë°©ë²•": "methods",
            "ë¹„ë²•": "secrets",
            "íŒ": "tips",
            "ê°€ì´ë“œ": "guide",
            "ë¼ì´í”„": "life",
            "ìŠ¤íƒ€ì¼": "style",
            "ê±´ê°•": "health",
            "ë‹¤ì´ì–´íŠ¸": "diet",
            "ìš´ë™": "exercise",
            "ëª…ìƒ": "meditation",
            "ì§‘ì¤‘": "focus",
            "ì‹œê°„": "time",
            "ê´€ë¦¬": "management",
            "ëˆ": "money",
            "íˆ¬ì": "investment",
            "ë¶€ì": "rich",
            "í–‰ë³µ": "happiness",
            "ì‚¬ë‘": "love",
            "ì¸ê°„ê´€ê³„": "relationships",
            "ìì‹ ê°": "confidence",
            "ë™ê¸°ë¶€ì—¬": "motivation",
            "ì˜ê°": "inspiration",
            "ì°½ì—…": "startup",
            "ë¹„ì¦ˆë‹ˆìŠ¤": "business",
            "ë§ˆì¼€íŒ…": "marketing",
            "ë¸Œëœë”©": "branding",
            "ì†Œì…œë¯¸ë””ì–´": "social media",
            "ìœ íŠœë¸Œ": "youtube",
            "ì½˜í…ì¸ ": "content",
            "í¬ë¦¬ì—ì´í„°": "creator"
        }
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë²ˆì—­ ì‹œë„
        words = text.split()
        translated_words = []
        for word in words:
            # íŠ¹ìˆ˜ë¬¸ì ì œê±°í•˜ê³  ë§¤í•‘ í™•ì¸
            clean_word = re.sub(r'[^\wê°€-í£]', '', word)
            if clean_word in simple_translations:
                translated_words.append(simple_translations[clean_word])
            else:
                # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
                found = False
                for ko, en in simple_translations.items():
                    if ko in clean_word:
                        translated_words.append(en)
                        found = True
                        break
                if not found:
                    translated_words.append(word)
        
        if translated_words and len(translated_words) > 0:
            fallback_result = " ".join(translated_words)
            if fallback_result != text:
                logger.info(f"Using fallback translation: '{text}' -> '{fallback_result}'")
                return fallback_result
    except Exception as e:
        logger.warning(f"Fallback translation failed: {e}")
    
    logger.warning("Translation failed or API error, returning original text.")
    return text

def translate_to_korean(text: str) -> str:
    if not text:
        return ""
    prompt = f"Translate the following English text into natural Korean. Return ONLY the translated text without any quotes, notes, or explanations:\n\n{text}"
    try:
        response = _generate_response(prompt)
        if response:
            return response.strip()
    except Exception as e:
        logger.error(f"Translation failed: {e}")
    return text

def translate_terms_to_korean(terms: List[str]) -> List[str]:
    if not terms:
        return []
    joined = ", ".join([t.strip() for t in terms if t])
    if not joined:
        return []
    prompt = f"Translate the following English keywords into Korean. Return ONLY a comma-separated list with no extra words:\n\n{joined}"
    try:
        response = _generate_response(prompt)
        if response:
            cleaned = response.replace("\n", ",").strip()
            out = [t.strip() for t in cleaned.split(",") if t.strip()]
            return out
    except Exception as e:
        logger.error(f"Translation failed: {e}")
    return terms

def translate_terms_to_english(terms: List[str]) -> List[str]:
    if not terms:
        return []
    joined = ", ".join([t.strip() for t in terms if t])
    if not joined:
        return []
    prompt = f"Translate the following keywords into natural English. Return ONLY a comma-separated list with no extra words:\n\n{joined}"
    try:
        response = _generate_response(prompt)
        if response:
            cleaned = response.replace("\n", ",").strip()
            out = [t.strip() for t in cleaned.split(",") if t.strip()]
            return out
    except Exception as e:
        logger.error(f"Translation failed: {e}")
    return terms
def generate_korean_terms(video_subject: str, video_script: str, amount: int = 5) -> List[str]:
    # Korean keyword generation for YouTube tags based on script content
    prompt = f"""
    ë‹¹ì‹ ì€ ì „ë¬¸ ìœ íŠœë¸Œ í¬ë¦¬ì—ì´í„°ì…ë‹ˆë‹¤.
    
    ì‘ì—…: ë‹¤ìŒ ì˜ìƒ ëŒ€ë³¸ì„ ë¶„ì„í•˜ì—¬ {amount}ê°œì˜ í•œêµ­ì–´ YouTube íƒœê·¸ë¥¼ ìƒì„±í•˜ì„¸ìš”.
    
    ìš”êµ¬ì‚¬í•­:
    1. ëŒ€ë³¸ì—ì„œ ì‹¤ì œë¡œ ì–¸ê¸‰ëœ ë‚´ìš©ë§Œ í‚¤ì›Œë“œë¡œ ì‚¬ìš©
    2. êµ¬ì²´ì ì´ê³  ê²€ìƒ‰ ê°€ëŠ¥í•œ í•œêµ­ì–´ ìš©ì–´
    3. ê° íƒœê·¸ëŠ” 1-3ë‹¨ì–´ë¡œ êµ¬ì„±
    4. ëŒ€ë³¸ì˜ í•µì‹¬ ë©”ì‹œì§€ì™€ ì§ì ‘ ê´€ë ¨ëœ í‚¤ì›Œë“œ
    5. "AIìƒì„±", "ë°”ì´ëŸ´", "ì½˜í…ì¸ ", "ì‡¼ì¸ " ê°™ì€ ì¼ë°˜ì  íƒœê·¸ ì‚¬ìš© ê¸ˆì§€
    6. ëŒ€ë³¸ì—ì„œ ì–¸ê¸‰ëœ í–‰ë™, ë°©ë²•, ê²°ê³¼, ê°ì •ì— ì§‘ì¤‘
    
    ë¶„ì„í•  ë‚´ìš©:
    ì£¼ì œ: {video_subject}
    ëŒ€ë³¸: {video_script[:1000]}
    
    ëŒ€ë³¸ì—ì„œ ì¶”ì¶œ ê°€ëŠ¥í•œ í‚¤ì›Œë“œ ì˜ˆì‹œ:
    - ì–¸ê¸‰ëœ êµ¬ì²´ì ì¸ í–‰ë™ì´ë‚˜ ë°©ë²•
    - ì œì‹œëœ ê²°ê³¼ë‚˜ íš¨ê³¼
    - ë‚˜íƒ€ë‚˜ëŠ” ê°ì •ì´ë‚˜ ìƒíƒœ
    - ì„¤ëª…ëœ ìƒí™©ì´ë‚˜ ë¬¸ì œ
    - ì œì•ˆëœ í•´ê²°ì±…ì´ë‚˜ íŒ
    
    ëŒ€ë³¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•œ {amount}ê°œì˜ í•œêµ­ì–´ í‚¤ì›Œë“œë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë‚˜ì—´í•˜ì„¸ìš”:
    """
    
    try:
        response = _generate_response(prompt)
        if response:
            # Clean up the response
            cleaned = response.strip()
            
            # Remove common prefixes
            prefixes = ["íƒœê·¸:", "í‚¤ì›Œë“œ:", "ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤", "ìƒì„±ëœ íƒœê·¸", "ë¶„ì„ ê²°ê³¼"]
            for p in prefixes:
                if cleaned.startswith(p):
                    cleaned = cleaned[len(p):].strip()
                    if cleaned.startswith(":"):
                        cleaned = cleaned[1:].strip()
            
            # Clean formatting
            cleaned = cleaned.replace("\n", ",").replace("- ", "").replace("* ", "").replace('"', '').strip()
            
            # Extract terms
            terms = [t.strip() for t in cleaned.split(",") if t.strip()]
            
            # Filter and validate Korean terms
            valid_terms = []
            for term in terms:
                term = term.strip()
                # Check if contains Korean characters and is reasonable length
                if (len(term.split()) <= 3 and len(term) > 1 and 
                    re.search("[ê°€-í£]", term) and 
                    # Exclude generic terms
                    not any(generic in term for generic in ["AIìƒì„±", "ë°”ì´ëŸ´", "ì½˜í…ì¸ ", "ì‡¼ì¸ ", "ì˜ìƒ", "ìœ íŠœë¸Œ"])):
                    valid_terms.append(term)
            
            if len(valid_terms) >= 3:
                logger.info(f"Generated script-based Korean tags: {valid_terms[:amount]}")
                return valid_terms[:amount]
                
    except Exception as e:
        logger.error(f"failed to generate Korean terms: {e}")
    
    # Enhanced fallback with script content analysis
    logger.warning("LLM failed to generate Korean terms. Using script analysis fallback.")
    return _generate_script_based_korean_keywords(video_subject, video_script, amount)

def _generate_script_based_korean_keywords(video_subject: str, video_script: str, amount: int = 5) -> List[str]:
    """ëŒ€ë³¸ ë‚´ìš© ë¶„ì„ ê¸°ë°˜ í•œêµ­ì–´ í‚¤ì›Œë“œ ìƒì„±"""
    logger.info(f"Analyzing Korean script content for keyword generation: '{video_subject}'")
    
    import re
    
    # ëŒ€ë³¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•œ íŒ¨í„´ ë¶„ì„
    script_lower = video_script.lower()
    subject_lower = video_subject.lower()
    
    # ëŒ€ë³¸ ë‚´ìš© ê¸°ë°˜ í•œêµ­ì–´ í‚¤ì›Œë“œ ë§¤í•‘ (êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì )
    content_keywords = {
        # í–‰ë™/ë°©ë²• ê´€ë ¨
        "ë¬¼ë§ˆì‹œê¸°": ["ë¬¼ë§ˆì‹œê¸°", "ìˆ˜ë¶„ì„­ì·¨", "ê±´ê°•ìŠµê´€"],
        "ìŠ¤íŠ¸ë ˆì¹­": ["ìŠ¤íŠ¸ë ˆì¹­", "ëª¸í’€ê¸°", "ìœ ì—°ì„±"],
        "ëª…ìƒ": ["ëª…ìƒ", "ë§ˆìŒì±™ê¹€", "ì •ì‹ ê±´ê°•"],
        "í˜¸í¡": ["í˜¸í¡ë²•", "ì‹¬í˜¸í¡", "ë§ˆìŒì•ˆì •"],
        "ê±·ê¸°": ["ê±·ê¸°ìš´ë™", "ì‚°ì±…", "ìœ ì‚°ì†Œ"],
        "ìš´ë™": ["ìš´ë™ë²•", "í—¬ìŠ¤", "ì²´ë ¥ê´€ë¦¬"],
        "ìš”ë¦¬": ["ìš”ë¦¬ë²•", "ìŒì‹ë§Œë“¤ê¸°", "ê±´ê°•ì‹"],
        "ë…ì„œ": ["ë…ì„œë²•", "ì±…ì½ê¸°", "ì§€ì‹ìŠµë“"],
        "ê¸°ë¡": ["ê¸°ë¡í•˜ê¸°", "ì¼ê¸°ì“°ê¸°", "ê³„íšì„¸ìš°ê¸°"],
        
        # ê²°ê³¼/íš¨ê³¼ ê´€ë ¨
        "ì§‘ì¤‘ë ¥": ["ì§‘ì¤‘ë ¥í–¥ìƒ", "ëª°ì…", "ìƒì‚°ì„±"],
        "íš¨ìœ¨": ["íš¨ìœ¨ì„±", "ì‹œê°„ê´€ë¦¬", "ìƒì‚°ì„±"],
        "ë³€í™”": ["ë³€í™”", "ê°œì„ ", "ì„±ì¥"],
        "ì„±ê³¼": ["ì„±ê³¼", "ê²°ê³¼", "ë‹¬ì„±"],
        "ê±´ê°•": ["ê±´ê°•ê´€ë¦¬", "ì›°ë¹™", "ì²´ë ¥"],
        
        # ê°ì •/ìƒíƒœ ê´€ë ¨
        "ìŠ¤íŠ¸ë ˆìŠ¤": ["ìŠ¤íŠ¸ë ˆìŠ¤ê´€ë¦¬", "ìŠ¤íŠ¸ë ˆìŠ¤í•´ì†Œ", "ë§ˆìŒê´€ë¦¬"],
        "í–‰ë³µ": ["í–‰ë³µ", "ê¸ì •", "ë§Œì¡±"],
        "ìì‹ ê°": ["ìì‹ ê°", "ìì¡´ê°", "ë§ˆì¸ë“œ"],
        "í‰ì˜¨": ["í‰ì˜¨", "ì•ˆì •", "íœ´ì‹"],
        
        # ì‹œê°„/ë£¨í‹´ ê´€ë ¨
        "ì•„ì¹¨": ["ì•„ì¹¨ë£¨í‹´", "ëª¨ë‹", "í•˜ë£¨ì‹œì‘"],
        "ì €ë…": ["ì €ë…ë£¨í‹´", "í•˜ë£¨ë§ˆë¬´ë¦¬", "íœ´ì‹"],
        "í•˜ë£¨": ["ì¼ìƒ", "ë£¨í‹´", "ìƒí™œíŒ¨í„´"],
        "ìŠµê´€": ["ì¢‹ì€ìŠµê´€", "ìŠµê´€ë§Œë“¤ê¸°", "ë¼ì´í”„ìŠ¤íƒ€ì¼"],
        
        # êµ¬ì²´ì  ë°©ë²•/íŒ
        "ë°©ë²•": ["ë°©ë²•", "ë…¸í•˜ìš°", "íŒ"],
        "ë¹„ë²•": ["ë¹„ë²•", "ê¿€íŒ", "ë…¸í•˜ìš°"],
        "ë‹¨ê³„": ["ë‹¨ê³„ë³„", "ìˆœì„œ", "ê³¼ì •"],
        "ì›ì¹™": ["ì›ì¹™", "ë²•ì¹™", "ê¸°ì¤€"]
    }
    
    # ëŒ€ë³¸ì—ì„œ ì–¸ê¸‰ëœ í‚¤ì›Œë“œ ì°¾ê¸°
    found_keywords = []
    
    # 1. ì§ì ‘ ë§¤ì¹­
    for korean_word, korean_keywords_list in content_keywords.items():
        if korean_word in script_lower or korean_word in subject_lower:
            found_keywords.extend(korean_keywords_list[:2])  # ê° ì¹´í…Œê³ ë¦¬ì—ì„œ 2ê°œì”©
            logger.info(f"Found Korean keyword '{korean_word}' -> {korean_keywords_list[:2]}")
    
    # 2. ë¬¸ë§¥ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ê°€
    context_keywords = []
    
    # ê±´ê°• ê´€ë ¨ ë¬¸ë§¥
    if any(word in script_lower for word in ["ê±´ê°•", "ëª¸", "ì²´ë ¥", "ìš´ë™", "ë‹¤ì´ì–´íŠ¸"]):
        context_keywords.extend(["ê±´ê°•ê´€ë¦¬", "ì›°ë¹™", "ì²´ë ¥í–¥ìƒ"])
    
    # ì„±ê³µ/ìê¸°ê³„ë°œ ë¬¸ë§¥
    if any(word in script_lower for word in ["ì„±ê³µ", "ëª©í‘œ", "ë‹¬ì„±", "ìŠµê´€", "ê³„ë°œ"]):
        context_keywords.extend(["ìê¸°ê³„ë°œ", "ì„±ê³µë²•", "ëª©í‘œë‹¬ì„±"])
    
    # ì¼ìƒ/ë¼ì´í”„ìŠ¤íƒ€ì¼ ë¬¸ë§¥
    if any(word in script_lower for word in ["ì¼ìƒ", "í•˜ë£¨", "ë£¨í‹´", "ìƒí™œ", "ì‹œê°„"]):
        context_keywords.extend(["ì¼ìƒë£¨í‹´", "ë¼ì´í”„ìŠ¤íƒ€ì¼", "ì‹œê°„ê´€ë¦¬"])
    
    # ë§ˆìŒ/ì •ì‹  ê±´ê°• ë¬¸ë§¥
    if any(word in script_lower for word in ["ë§ˆìŒ", "ì •ì‹ ", "ê°ì •", "ìŠ¤íŠ¸ë ˆìŠ¤", "í–‰ë³µ"]):
        context_keywords.extend(["ë§ˆìŒê´€ë¦¬", "ì •ì‹ ê±´ê°•", "ê°ì •ì¡°ì ˆ"])
    
    # 3. í‚¤ì›Œë“œ ì¡°í•© ë° ì •ë¦¬
    all_keywords = found_keywords + context_keywords
    
    # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
    unique_keywords = []
    seen = set()
    
    for keyword in all_keywords:
        keyword_clean = keyword.strip()
        if (keyword_clean not in seen and len(keyword_clean) > 1 and
            # ì¼ë°˜ì ì¸ íƒœê·¸ ì œì™¸
            not any(generic in keyword_clean for generic in ["AIìƒì„±", "ë°”ì´ëŸ´", "ì½˜í…ì¸ ", "ì‡¼ì¸ ", "ì˜ìƒ"])):
            unique_keywords.append(keyword_clean)
            seen.add(keyword_clean)
    
    # 4. ë¶€ì¡±í•œ ê²½ìš° ì£¼ì œ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ê°€
    if len(unique_keywords) < amount:
        subject_based = _get_korean_subject_keywords(video_subject)
        for keyword in subject_based:
            if keyword not in seen and len(unique_keywords) < amount:
                unique_keywords.append(keyword)
                seen.add(keyword)
    
    result = unique_keywords[:amount]
    logger.info(f"Final script-based Korean keywords: {result}")
    return result

def _get_korean_subject_keywords(video_subject: str) -> List[str]:
    """ì£¼ì œ ê¸°ë°˜ í•œêµ­ì–´ í‚¤ì›Œë“œ ìƒì„±"""
    subject_lower = video_subject.lower()
    
    subject_mapping = {
        "ì•„ì¹¨": ["ì•„ì¹¨ë£¨í‹´", "ëª¨ë‹", "í•˜ë£¨ì‹œì‘", "ê¸°ìƒ"],
        "ë£¨í‹´": ["ì¼ìƒë£¨í‹´", "ìƒí™œíŒ¨í„´", "ìŠµê´€", "ë¼ì´í”„ìŠ¤íƒ€ì¼"],
        "ìŠµê´€": ["ì¢‹ì€ìŠµê´€", "ìŠµê´€ë§Œë“¤ê¸°", "ìê¸°ê´€ë¦¬", "ë¼ì´í”„ìŠ¤íƒ€ì¼"],
        "ê±´ê°•": ["ê±´ê°•ê´€ë¦¬", "ì›°ë¹™", "ì²´ë ¥", "ê±´ê°•ë²•"],
        "ìš´ë™": ["ìš´ë™ë²•", "í—¬ìŠ¤", "ì²´ë ¥ê´€ë¦¬", "í”¼íŠ¸ë‹ˆìŠ¤"],
        "ë‹¤ì´ì–´íŠ¸": ["ë‹¤ì´ì–´íŠ¸", "ì²´ì¤‘ê´€ë¦¬", "ê±´ê°•ì‹", "ìš´ë™ë²•"],
        "ëˆ": ["ì¬í…Œí¬", "ëˆê´€ë¦¬", "ê²½ì œ", "íˆ¬ì"],
        "íˆ¬ì": ["íˆ¬ìë²•", "ì¬í…Œí¬", "ê²½ì œ", "ìì‚°ê´€ë¦¬"],
        "ì„±ê³µ": ["ì„±ê³µë²•", "ìê¸°ê³„ë°œ", "ëª©í‘œë‹¬ì„±", "ì„±ì¥"],
        "ìŠ¤íŠ¸ë ˆìŠ¤": ["ìŠ¤íŠ¸ë ˆìŠ¤ê´€ë¦¬", "ë§ˆìŒê´€ë¦¬", "íë§", "íœ´ì‹"],
        "ì‹œê°„": ["ì‹œê°„ê´€ë¦¬", "íš¨ìœ¨ì„±", "ìƒì‚°ì„±", "ê³„íš"],
        "ê´€ë¦¬": ["ìê¸°ê´€ë¦¬", "ìƒí™œê´€ë¦¬", "íš¨ìœ¨ì„±", "ì¡°ì ˆ"],
        "ë°©ë²•": ["ë…¸í•˜ìš°", "íŒ", "ë¹„ë²•", "í•´ê²°ì±…"],
        "ë§ˆìŒ": ["ë§ˆìŒê´€ë¦¬", "ì •ì‹ ê±´ê°•", "ê°ì •", "ì‹¬ë¦¬"]
    }
    
    keywords = []
    for korean, korean_list in subject_mapping.items():
        if korean in subject_lower:
            keywords.extend(korean_list)
    
    return keywords[:8] if keywords else ["ì •ë³´", "íŒ", "ë…¸í•˜ìš°", "ì¼ìƒ"]


def _remove_greetings(script: str, language: str = "auto") -> str:
    """ëŒ€ë³¸ì—ì„œ ì¸ì‚¬ë§ ì œê±°"""
    if not script:
        return script
    
    lines = script.split('\n')
    cleaned_lines = []
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # í•œêµ­ì–´ ì¸ì‚¬ë§ íŒ¨í„´
        korean_greetings = [
            "ì•ˆë…•í•˜ì„¸ìš”", "ì•ˆë…•", "ì—¬ëŸ¬ë¶„", "ì‹œì²­ì ì—¬ëŸ¬ë¶„", "êµ¬ë…ì ì—¬ëŸ¬ë¶„",
            "ì˜¤ëŠ˜ì€", "ì˜¤ëŠ˜ ì˜ìƒì—ì„œëŠ”", "ì´ë²ˆ ì˜ìƒì—ì„œëŠ”", "ë°˜ê°‘ìŠµë‹ˆë‹¤",
            "í™˜ì˜í•©ë‹ˆë‹¤", "ë‹¤ì‹œ ë§Œë‚˜ëµ™ìŠµë‹ˆë‹¤", "ì±„ë„ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤",
            "ì˜¤ëŠ˜ë„", "ë‹¤ì‹œ í•œë²ˆ"
        ]
        
        # ì˜ì–´ ì¸ì‚¬ë§ íŒ¨í„´
        english_greetings = [
            "hello", "hi everyone", "hi there", "welcome", "welcome back",
            "good morning", "good afternoon", "good evening", "hey guys",
            "what's up", "greetings", "welcome to", "hey there", "hi folks",
            "today we", "in today's video", "today i", "today's topic"
        ]
        
        # ì¸ì‚¬ë§ë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì¥ ì œê±°
        should_skip = False
        line_lower = line.lower()
        
        if language == "ko-KR" or language == "auto":
            for greeting in korean_greetings:
                if line.startswith(greeting) or greeting in line[:30]:
                    should_skip = True
                    break
        
        if language == "en-US" or not should_skip:
            for greeting in english_greetings:
                if (line_lower.startswith(greeting + " ") or 
                    line_lower.startswith(greeting + ",") or
                    line_lower.startswith(greeting + ".") or
                    line_lower == greeting):
                    should_skip = True
                    break
        
        # ì¸ì‚¬ë§ì´ ì•„ë‹Œ ë¬¸ì¥ë§Œ ì¶”ê°€
        if not should_skip:
            cleaned_lines.append(line)
    
    # ê²°ê³¼ ì¡°í•©
    result = '\n'.join(cleaned_lines).strip()
    
    # ë¹ˆ ê²°ê³¼ì¸ ê²½ìš° ì›ë³¸ ë°˜í™˜ (ëª¨ë“  ë¬¸ì¥ì´ ì¸ì‚¬ë§ì¸ ê²½ìš° ë°©ì§€)
    if not result or len(result) < 20:
        # ì²« ë²ˆì§¸ ë¬¸ì¥ë§Œ ì œê±°í•˜ê³  ë‚˜ë¨¸ì§€ ë°˜í™˜
        original_lines = script.split('\n')
        if len(original_lines) > 1:
            return '\n'.join(original_lines[1:]).strip()
        else:
            return script
    
    return result
def _clean_markdown_formatting(script: str) -> str:
    """ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì œê±°"""
    if not script:
        return script
    
    import re
    
    # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì œê±°
    # ** ë³¼ë“œ ì œê±°
    script = re.sub(r'\*\*(.*?)\*\*', r'\1', script)
    
    # * ì´íƒ¤ë¦­ ì œê±°
    script = re.sub(r'\*(.*?)\*', r'\1', script)
    
    # ## í—¤ë” ì œê±°
    script = re.sub(r'^#{1,6}\s*', '', script, flags=re.MULTILINE)
    
    # - ë¦¬ìŠ¤íŠ¸ ì œê±°
    script = re.sub(r'^-\s*', '', script, flags=re.MULTILINE)
    
    # 1. ìˆ«ì ë¦¬ìŠ¤íŠ¸ ì œê±°
    script = re.sub(r'^\d+\.\s*', '', script, flags=re.MULTILINE)
    
    # [ë§í¬](url) í˜•ì‹ ì œê±°
    script = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', script)
    
    # ``` ì½”ë“œ ë¸”ë¡ ì œê±°
    script = re.sub(r'```.*?```', '', script, flags=re.DOTALL)
    
    # ` ì¸ë¼ì¸ ì½”ë“œ ì œê±°
    script = re.sub(r'`([^`]+)`', r'\1', script)
    
    # > ì¸ìš©ë¬¸ ì œê±°
    script = re.sub(r'^>\s*', '', script, flags=re.MULTILINE)
    
    # [ì¥ë©´ 1], [ì¥ë©´ 2] ë“± ì¥ë©´ ì„¤ëª… ì œê±°
    script = re.sub(r'\[ì¥ë©´\s*\d+\]', '', script)
    script = re.sub(r'\[Scene\s*\d+\]', '', script)
    
    # ì—°ì†ëœ ê³µë°± ì •ë¦¬
    script = re.sub(r'\s+', ' ', script)
    
    # ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì •ë¦¬
    script = re.sub(r'\n\s*\n', '\n\n', script)
    
    return script.strip()
def _generate_script_based_keywords(video_subject: str, video_script: str, amount: int = 5) -> List[str]:
    """ëŒ€ë³¸ ë‚´ìš© ë¶„ì„ ê¸°ë°˜ í‚¤ì›Œë“œ ìƒì„±"""
    logger.info(f"Analyzing script content for keyword generation: '{video_subject}'")
    
    import re
    
    # ëŒ€ë³¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•œ íŒ¨í„´ ë¶„ì„
    script_lower = video_script.lower()
    subject_lower = video_subject.lower()
    
    # ëŒ€ë³¸ ë‚´ìš© ê¸°ë°˜ í‚¤ì›Œë“œ ë§¤í•‘ (ë” êµ¬ì²´ì ì´ê³  ì‹œê°ì )
    content_keywords = {
        # í–‰ë™/ë™ì‘ ê´€ë ¨
        "ë¬¼": ["water", "drinking", "hydration"],
        "ìŠ¤íŠ¸ë ˆì¹­": ["stretching", "exercise", "flexibility"],
        "ëª…ìƒ": ["meditation", "mindfulness", "relaxation"],
        "í˜¸í¡": ["breathing", "meditation", "calm"],
        "ê±·ê¸°": ["walking", "outdoor", "exercise"],
        "ìš´ë™": ["workout", "fitness", "gym"],
        "ìš”ë¦¬": ["cooking", "kitchen", "food preparation"],
        "ë…ì„œ": ["reading", "books", "learning"],
        "ì“°ê¸°": ["writing", "notebook", "planning"],
        "ê¸°ë¡": ["writing", "journal", "planning"],
        
        # ë¬¼ê±´/ë„êµ¬ ê´€ë ¨
        "ì»¤í”¼": ["coffee", "morning", "cafe"],
        "ì±…": ["books", "reading", "education"],
        "ë…¸íŠ¸": ["notebook", "writing", "planning"],
        "ìŠ¤ë§ˆíŠ¸í°": ["smartphone", "technology", "digital"],
        "ì»´í“¨í„°": ["computer", "technology", "work"],
        "ëˆ": ["money", "finance", "cash"],
        "í†µì¥": ["banking", "finance", "savings"],
        "ì¹´ë“œ": ["credit card", "payment", "finance"],
        
        # ì¥ì†Œ/í™˜ê²½ ê´€ë ¨
        "ì§‘": ["home", "house", "indoor"],
        "ì‚¬ë¬´ì‹¤": ["office", "workplace", "business"],
        "ì¹´í˜": ["cafe", "coffee shop", "social"],
        "ê³µì›": ["park", "outdoor", "nature"],
        "í—¬ìŠ¤ì¥": ["gym", "fitness", "workout"],
        "ë¶€ì—Œ": ["kitchen", "cooking", "home"],
        
        # ê°ì •/ìƒíƒœ ê´€ë ¨
        "ìŠ¤íŠ¸ë ˆìŠ¤": ["stress", "pressure", "tension"],
        "í–‰ë³µ": ["happiness", "joy", "positive"],
        "í”¼ê³¤": ["tired", "fatigue", "rest"],
        "ì§‘ì¤‘": ["focus", "concentration", "productivity"],
        "ì„±ê³µ": ["success", "achievement", "goal"],
        "ì‹¤íŒ¨": ["failure", "challenge", "learning"],
        
        # ì‹œê°„ ê´€ë ¨
        "ì•„ì¹¨": ["morning", "sunrise", "early"],
        "ì €ë…": ["evening", "sunset", "night"],
        "í•˜ë£¨": ["daily", "routine", "lifestyle"],
        "ì£¼ë§": ["weekend", "leisure", "relaxation"],
        
        # ê²°ê³¼/íš¨ê³¼ ê´€ë ¨
        "ë³€í™”": ["change", "transformation", "improvement"],
        "ì„±ì¥": ["growth", "development", "progress"],
        "íš¨ê³¼": ["results", "benefits", "improvement"],
        "ê±´ê°•": ["health", "wellness", "vitality"]
    }
    
    # ëŒ€ë³¸ì—ì„œ ì–¸ê¸‰ëœ í‚¤ì›Œë“œ ì°¾ê¸°
    found_keywords = []
    
    # 1. ì§ì ‘ ë§¤ì¹­
    for korean_word, english_keywords in content_keywords.items():
        if korean_word in script_lower or korean_word in subject_lower:
            found_keywords.extend(english_keywords[:2])  # ê° ì¹´í…Œê³ ë¦¬ì—ì„œ 2ê°œì”©
            logger.info(f"Found keyword '{korean_word}' -> {english_keywords[:2]}")
    
    # 2. ë¬¸ë§¥ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ê°€
    context_keywords = []
    
    # ê±´ê°• ê´€ë ¨ ë¬¸ë§¥
    if any(word in script_lower for word in ["ê±´ê°•", "ëª¸", "ì²´ë ¥", "ìš´ë™", "ë‹¤ì´ì–´íŠ¸"]):
        context_keywords.extend(["healthy lifestyle", "wellness", "fitness"])
    
    # ì„±ê³µ/ìê¸°ê³„ë°œ ë¬¸ë§¥
    if any(word in script_lower for word in ["ì„±ê³µ", "ëª©í‘œ", "ë‹¬ì„±", "ìŠµê´€", "ê³„ë°œ"]):
        context_keywords.extend(["success", "achievement", "personal growth"])
    
    # ëˆ/ì¬ì • ë¬¸ë§¥
    if any(word in script_lower for word in ["ëˆ", "íˆ¬ì", "ì €ì¶•", "ì¬í…Œí¬", "ê²½ì œ"]):
        context_keywords.extend(["money", "finance", "investment"])
    
    # ì¼ìƒ/ë¼ì´í”„ìŠ¤íƒ€ì¼ ë¬¸ë§¥
    if any(word in script_lower for word in ["ì¼ìƒ", "í•˜ë£¨", "ë£¨í‹´", "ìƒí™œ", "ì‹œê°„"]):
        context_keywords.extend(["daily routine", "lifestyle", "time management"])
    
    # 3. ëŒ€ë³¸ì—ì„œ êµ¬ì²´ì ì¸ í–‰ë™ ì¶”ì¶œ
    action_patterns = [
        (r"(\w+)ì„?\s*ë§ˆì‹œ", "drinking"),
        (r"(\w+)ì„?\s*ë¨¹", "eating"),
        (r"(\w+)ì„?\s*í•˜", "doing"),
        (r"(\w+)ì„?\s*ë³´", "watching"),
        (r"(\w+)ì„?\s*ì“°", "writing"),
        (r"(\w+)ì„?\s*ì½", "reading"),
    ]
    
    for pattern, action in action_patterns:
        matches = re.findall(pattern, script_lower)
        if matches:
            context_keywords.append(action)
    
    # 4. í‚¤ì›Œë“œ ì¡°í•© ë° ì •ë¦¬
    all_keywords = found_keywords + context_keywords
    
    # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
    unique_keywords = []
    seen = set()
    
    for keyword in all_keywords:
        keyword_clean = keyword.lower().strip()
        if keyword_clean not in seen and len(keyword_clean) > 2:
            unique_keywords.append(keyword_clean)
            seen.add(keyword_clean)
    
    # 5. ë¶€ì¡±í•œ ê²½ìš° ì£¼ì œ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ê°€
    if len(unique_keywords) < amount:
        subject_based = _get_subject_based_keywords(video_subject)
        for keyword in subject_based:
            if keyword.lower() not in seen and len(unique_keywords) < amount:
                unique_keywords.append(keyword.lower())
                seen.add(keyword.lower())
    
    # 6. ì—¬ì „íˆ ë¶€ì¡±í•œ ê²½ìš° ì¼ë°˜ í‚¤ì›Œë“œ ì¶”ê°€
    if len(unique_keywords) < amount:
        general_keywords = ["lifestyle", "people", "modern", "daily", "routine"]
        for keyword in general_keywords:
            if keyword not in seen and len(unique_keywords) < amount:
                unique_keywords.append(keyword)
                seen.add(keyword)
    
    result = unique_keywords[:amount]
    logger.info(f"Final script-based keywords: {result}")
    return result

def _get_subject_based_keywords(video_subject: str) -> List[str]:
    """ì£¼ì œ ê¸°ë°˜ í‚¤ì›Œë“œ ìƒì„±"""
    subject_lower = video_subject.lower()
    
    subject_mapping = {
        "ì•„ì¹¨": ["morning", "sunrise", "breakfast", "routine"],
        "ë£¨í‹´": ["routine", "daily", "habit", "lifestyle"],
        "ìŠµê´€": ["habit", "routine", "lifestyle", "daily"],
        "ê±´ê°•": ["health", "wellness", "fitness", "nutrition"],
        "ìš´ë™": ["exercise", "workout", "fitness", "gym"],
        "ë‹¤ì´ì–´íŠ¸": ["diet", "weight loss", "healthy eating", "nutrition"],
        "ëˆ": ["money", "finance", "cash", "wealth"],
        "íˆ¬ì": ["investment", "finance", "money", "trading"],
        "ì„±ê³µ": ["success", "achievement", "goal", "winner"],
        "ìŠ¤íŠ¸ë ˆìŠ¤": ["stress", "relaxation", "calm", "meditation"],
        "ì‹œê°„": ["time", "clock", "schedule", "planning"],
        "ê´€ë¦¬": ["management", "organization", "planning", "control"],
        "ë°©ë²•": ["method", "way", "technique", "approach"],
        "ë¹„ë²•": ["secret", "tip", "technique", "method"]
    }
    
    keywords = []
    for korean, english_list in subject_mapping.items():
        if korean in subject_lower:
            keywords.extend(english_list)
    
    return keywords[:8] if keywords else ["lifestyle", "people", "modern", "daily"]